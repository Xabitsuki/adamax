{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Movie behind a Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import findspark\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as psf\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.10:0.4.1 pyspark-shell'\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set('spark.sql.session.timeZone', 'UTC')\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of datasets\n",
    "\n",
    "The OpenSubtitles dataset is a compressed cluster of folders containing XML files. Each XML file is split into a script portion with the subtitles of the movie and a metadata portion with additional information about the movie or show. The name of one of the parent folders of the XML file is the corresponding IMDb identifier of the movie or show, thus allowing us to extract additional information from the IMDb dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDb Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have at our disposal the IMDb ratings and basics dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO scrape data https://datasets.imdbws.com/\n",
    "ratings_fn = \"title.ratings.tsv.gz\"\n",
    "basics_fn = \"title.basics.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------+\n",
      "|   tconst|averageRating|numVotes|\n",
      "+---------+-------------+--------+\n",
      "|tt0000001|          5.8|    1440|\n",
      "|tt0000002|          6.3|     172|\n",
      "|tt0000003|          6.6|    1041|\n",
      "|tt0000004|          6.4|     102|\n",
      "|tt0000005|          6.2|    1735|\n",
      "|tt0000006|          5.5|      91|\n",
      "|tt0000007|          5.5|     579|\n",
      "|tt0000008|          5.6|    1539|\n",
      "|tt0000009|          5.6|      74|\n",
      "|tt0000010|          6.9|    5127|\n",
      "|tt0000011|          5.4|     214|\n",
      "|tt0000012|          7.4|    8599|\n",
      "|tt0000013|          5.7|    1318|\n",
      "|tt0000014|          7.2|    3739|\n",
      "|tt0000015|          6.2|     660|\n",
      "|tt0000016|          5.9|     982|\n",
      "|tt0000017|          4.8|     197|\n",
      "|tt0000018|          5.5|     414|\n",
      "|tt0000019|          6.6|      13|\n",
      "|tt0000020|          5.1|     232|\n",
      "+---------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ratings = spark.read.option(\"header\", \"true\")\\\n",
    "                       .option(\"sep\", \"\\t\")\\\n",
    "                       .csv(\"imdb_data/\" + ratings_fn)\n",
    "df_ratings = df_ratings.selectExpr(\"tconst\", \n",
    "                                   \"cast(averageRating as float) averageRating\", \n",
    "                                   \"cast(numVotes as int) numVotes\")\n",
    "df_ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|              genres|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|tt0000001|    short|          Carmencita|          Carmencita|      0|     1894|     \\N|             1|   Documentary,Short|\n",
      "|tt0000002|    short|Le clown et ses c...|Le clown et ses c...|      0|     1892|     \\N|             5|     Animation,Short|\n",
      "|tt0000003|    short|      Pauvre Pierrot|      Pauvre Pierrot|      0|     1892|     \\N|             4|Animation,Comedy,...|\n",
      "|tt0000004|    short|         Un bon bock|         Un bon bock|      0|     1892|     \\N|            \\N|     Animation,Short|\n",
      "|tt0000005|    short|    Blacksmith Scene|    Blacksmith Scene|      0|     1893|     \\N|             1|        Comedy,Short|\n",
      "|tt0000006|    short|   Chinese Opium Den|   Chinese Opium Den|      0|     1894|     \\N|             1|               Short|\n",
      "|tt0000007|    short|Corbett and Court...|Corbett and Court...|      0|     1894|     \\N|             1|         Short,Sport|\n",
      "|tt0000008|    short|Edison Kinetoscop...|Edison Kinetoscop...|      0|     1894|     \\N|             1|   Documentary,Short|\n",
      "|tt0000009|    movie|          Miss Jerry|          Miss Jerry|      0|     1894|     \\N|            45|             Romance|\n",
      "|tt0000010|    short|Employees Leaving...|La sortie de l'us...|      0|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000011|    short|Akrobatisches Pot...|Akrobatisches Pot...|      0|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000012|    short|The Arrival of a ...|L'arrivée d'un tr...|      0|     1896|     \\N|             1|   Documentary,Short|\n",
      "|tt0000013|    short|The Photographica...|Neuville-sur-Saôn...|      0|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000014|    short|Tables Turned on ...|   L'arroseur arrosé|      0|     1895|     \\N|             1|        Comedy,Short|\n",
      "|tt0000015|    short| Autour d'une cabine| Autour d'une cabine|      0|     1894|     \\N|             2|     Animation,Short|\n",
      "|tt0000016|    short|Barque sortant du...|Barque sortant du...|      0|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000017|    short|Italienischer Bau...|Italienischer Bau...|      0|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000018|    short|Das boxende Känguruh|Das boxende Känguruh|      0|     1895|     \\N|             1|               Short|\n",
      "|tt0000019|    short|    The Clown Barber|    The Clown Barber|      0|     1898|     \\N|            \\N|        Comedy,Short|\n",
      "|tt0000020|    short|      The Derby 1895|      The Derby 1895|      0|     1895|     \\N|             1|Documentary,Short...|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_basics = spark.read.option(\"header\", \"true\")\\\n",
    "                      .option(\"sep\", \"\\t\")\\\n",
    "                      .csv(\"imdb_data/\" + basics_fn)\n",
    "df_basics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenSubtitles dataset\n",
    "\n",
    "The dataset consists of 31 GB of XML files distributed in the following file structure: \n",
    "\n",
    "```\n",
    "├── opensubtitle\n",
    "│   ├── OpenSubtitles2018\n",
    "│   │   ├── Year\n",
    "│   │   │   ├── Id\n",
    "│   │   │   │   ├── #######.xml.gz\n",
    "│   │   │   │   ├── #######.xml.gz\n",
    "│   ├── en.tar.gz\n",
    "│   ├── fr.tar.gz\n",
    "│   ├── zh_cn.tar.gz\n",
    "```\n",
    "where\n",
    "- `######` is a 6-digit unique identifier of the file on the OpenSubtitles dataset.\n",
    "- `Year` is the year the movie or episode was made.\n",
    "- `Id` is a 5 to 7 digit identifier (if it's 7-digit it's also an IMDb identifier).\n",
    "\n",
    "The subtitles are provided in different languages. We only analyze the `OpenSubtitles2018` folder and it's the only folder we detail.\n",
    "\n",
    "The decompressed XML files vary in size, ranging from 5KB to 9000KB sized files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML Files\n",
    "\n",
    "Each XML file is split into a `document` and `metadata` section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtitles\n",
    "\n",
    "The `document` section contains all the subtitles and its general structure is the following:\n",
    "\n",
    "```\n",
    "├── s\n",
    "│   ├── time: Integer\n",
    "│   ├── w: String\n",
    "```\n",
    "\n",
    "An example snippet of an XML file:\n",
    "\n",
    "```xml\n",
    "  <s id=\"1\">\n",
    "    <time id=\"T1S\" value=\"00:00:51,819\" />\n",
    "    <w id=\"1.1\">Travis</w>\n",
    "    <w id=\"1.2\">.</w>\n",
    "    <time id=\"T1E\" value=\"00:00:53,352\" />\n",
    "  </s>\n",
    "```\n",
    "\n",
    "The subtitles in each XML file are stored by blocks denoted by `s` with a unique `id` attribute (integers in increasing order starting at 1).  \n",
    "\n",
    "Each block (`<s id=\"1\">` for instance) has a:  \n",
    "\n",
    "1. Set of timestamps (denoted by `time`) with\n",
    " - A timestamp `id` attribute that can take two different formats: `T#S` or `T#E`, where _S_ indicates _start_, _E_ indicates _end_ and _#_ is an increasing integer. \n",
    " - A `value` attribute which has the format `HH:mm:ss,fff`.\n",
    "\n",
    "2. Set of words (denoted by `w`) with\n",
    " - an `id` attribute that is simply an increasing number of decimal numbers of the format `X.Y` where X is the string id and Y is the word id within the corresponding string\n",
    " - a non-empty `value` attribute that contains a token: a word or a punctuation character. \n",
    "\n",
    "It sometimes also has an `alternative`, `initial` and `emphasis` attribute.  \n",
    "\n",
    " - The `initial` attribute generally corresponds to slang words or mispronounced words because of an accent such as _lyin'_ instead of _lying_.  \n",
    " - The `alternative` attribute is another way of displaying the subtitle for example _HOW_ instead of _how_.\n",
    " - The `emphasis` attribute is a boolean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "The `metadata` section has the following structure:\n",
    "\n",
    "```\n",
    "├── Conversion\n",
    "│   ├── corrected_words: Integer\n",
    "│   ├── sentences: Integer\n",
    "│   ├── tokens: Integer\n",
    "│   ├── encoding: String (always utf-8)\n",
    "│   ├── unknown_words: Integer\n",
    "│   ├── ignored_blocks: Integer\n",
    "│   ├── truecased_words: Integer\n",
    "├── Subtitle\n",
    "│   ├── language: String\n",
    "│   ├── date: String\n",
    "│   ├── duration: String\n",
    "│   ├── cds: String (presented as #/# where # is an int)\n",
    "│   ├── blocks: Integer\n",
    "│   ├── confidence: Double\n",
    "├── Source\n",
    "│   ├── genre: String[] (up to 3 genres)\n",
    "│   ├── year: Integer\n",
    "│   ├── duration: Integer (in minutes)\n",
    "│   ├── original: String\n",
    "│   ├── country: String\n",
    "```\n",
    "\n",
    "We note that some XML files may not have all the entries. \n",
    "We can use the metadata to obtain additional information about the movie or show's subtitles and compute certain statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_document = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "                                    .options(rowTag='document') \\\n",
    "                                    .load('sample_dataset/2017/6464116/6887453.xml.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- conversion: struct (nullable = true)\n",
      " |    |-- corrected_words: long (nullable = true)\n",
      " |    |-- encoding: string (nullable = true)\n",
      " |    |-- ignored_blocks: long (nullable = true)\n",
      " |    |-- sentences: long (nullable = true)\n",
      " |    |-- tokens: long (nullable = true)\n",
      " |    |-- truecased_words: long (nullable = true)\n",
      " |    |-- unknown_words: long (nullable = true)\n",
      " |-- source: struct (nullable = true)\n",
      " |    |-- duration: long (nullable = true)\n",
      " |    |-- genre: string (nullable = true)\n",
      " |    |-- year: long (nullable = true)\n",
      " |-- subtitle: struct (nullable = true)\n",
      " |    |-- blocks: long (nullable = true)\n",
      " |    |-- cds: string (nullable = true)\n",
      " |    |-- confidence: double (nullable = true)\n",
      " |    |-- date: string (nullable = true)\n",
      " |    |-- duration: string (nullable = true)\n",
      " |    |-- language: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|          conversion|              source|            subtitle|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[0, utf-8, 2, 967...|[41, Action,Crime...|[888, 1/1, 1.0, 2...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample_metadata = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "                                    .options(rowTag='meta') \\\n",
    "                                    .load('sample_dataset/2017/6464116/6887453.xml.gz')\n",
    "\n",
    "df_sample_metadata.printSchema()\n",
    "df_sample_metadata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the schema. We need to decide what is actually relevant for us to filter out the useless information and choose which format our dataframe should have (for example having all the different genres in a separate column.)\n",
    "\n",
    "For the metadata we have a very clean dataframe which can be used for a lot of statistics and filtering : filtering by genre, by year etc. \n",
    "\n",
    "We can see that there is no actual link between our dataframes (subtitles and metadata) : neither contain the movie id that would pair them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "Going through the dataset we notice a few things:\n",
    "\n",
    "1. The dataset has meaningless folders. For example, the folder 1858/ is empty.\n",
    "2. Dataset contains XML files that are not related to movies or TV shows. For example, the folder 666/ contains Justin Bieber song subtitles.  \n",
    "3. Trailer of films can be present in the dataset. For example, the folder 2018/ we found for example Black Panther teaser trailer subtitles.\n",
    "4. Each movie might have more than 1 subtitle file.\n",
    "5. Some subtitle files contain text that is not related to the movie, like credits to the person who made the subtitles.\n",
    "6. The IDMDb folder name is not always a 7-digit number, meaning it is not always a valid IMDb identifer and we can't retrieve the IMDb info.\n",
    "7. Each block may have an arbitrary number (including 0) of timestamps associated to it.\n",
    "\n",
    "To solve points 1 and 2, we ignore all the folders which aren't inside the range of 1920-2018.\n",
    "\n",
    "To solve point 3, we drop trailers by looking at the `duration` field in the metadata section.\n",
    "\n",
    "To solve point 4, we simply take the first one.\n",
    "\n",
    "To solve point 6, we keep movies that have a correct IMDb identifier. Hence, all the files in folders that don't have a 7-digit folder name are dropped.\n",
    "\n",
    "To solve point 7, we decide not to associate a timestamp to each word for the moment.\n",
    " \n",
    "For the moment, we take a sample of the dataset from the cluster (see python script `extract_sample_2.py`) by collecting 1 or 2 movies for each year in the range 1920-2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing an analysis of the files and considering the statistics we want to obtain taking the size of our data into account, we decide to load the metadata and subtitles directly into 1 dataframe where we manipulate it as before. We decide not to extract all tokens at first as it would induce into very heavy computations. We store the text in an array of subtitles where each subtitle is an array of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: long (nullable = true)\n",
      " |-- meta: struct (nullable = true)\n",
      " |    |-- conversion: struct (nullable = true)\n",
      " |    |    |-- corrected_words: long (nullable = true)\n",
      " |    |    |-- encoding: string (nullable = true)\n",
      " |    |    |-- ignored_blocks: long (nullable = true)\n",
      " |    |    |-- sentences: long (nullable = true)\n",
      " |    |    |-- tokens: long (nullable = true)\n",
      " |    |    |-- truecased_words: long (nullable = true)\n",
      " |    |    |-- unknown_words: long (nullable = true)\n",
      " |    |-- source: struct (nullable = true)\n",
      " |    |    |-- duration: long (nullable = true)\n",
      " |    |    |-- genre: string (nullable = true)\n",
      " |    |    |-- year: long (nullable = true)\n",
      " |    |-- subtitle: struct (nullable = true)\n",
      " |    |    |-- blocks: long (nullable = true)\n",
      " |    |    |-- cds: string (nullable = true)\n",
      " |    |    |-- confidence: double (nullable = true)\n",
      " |    |    |-- date: string (nullable = true)\n",
      " |    |    |-- duration: string (nullable = true)\n",
      " |    |    |-- language: string (nullable = true)\n",
      " |-- s: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _emphasis: boolean (nullable = true)\n",
      " |    |    |-- _id: long (nullable = true)\n",
      " |    |    |-- time: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |-- _id: string (nullable = true)\n",
      " |    |    |    |    |-- _value: string (nullable = true)\n",
      " |    |    |-- w: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |-- _alternative: string (nullable = true)\n",
      " |    |    |    |    |-- _emphasis: boolean (nullable = true)\n",
      " |    |    |    |    |-- _id: double (nullable = true)\n",
      "\n",
      "+-------+--------------------+--------------------+\n",
      "|    _id|                meta|                   s|\n",
      "+-------+--------------------+--------------------+\n",
      "|6887453|[[0, utf-8, 2, 96...|[[, 1, [[, T1S, 0...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_id = '6464116'\n",
    "df_document_example = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "                                     .options(rowTag='document') \\\n",
    "                                     .load('sample_dataset/2017/6464116/6887453.xml.gz')\n",
    "df_document_example.printSchema()\n",
    "df_document_example.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtitles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _emphasis: boolean (nullable = true)\n",
      " |-- _id: long (nullable = true)\n",
      " |-- time: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _id: string (nullable = true)\n",
      " |    |    |-- _value: string (nullable = true)\n",
      " |-- w: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _alternative: string (nullable = true)\n",
      " |    |    |-- _emphasis: boolean (nullable = true)\n",
      " |    |    |-- _id: double (nullable = true)\n",
      "\n",
      "+---------+---+--------------------+--------------------+\n",
      "|_emphasis|_id|                time|                   w|\n",
      "+---------+---+--------------------+--------------------+\n",
      "|     null|  1|[[, T1S, 00:00:01...|[[[,,, 1.1], [sho...|\n",
      "|     null|  2|[[, T2S, 00:00:09...|[[[,,, 2.1], [buz...|\n",
      "|     true|  3|[[, T4S, 00:00:12...|[[Clear,,, 3.1], ...|\n",
      "|     null|  4|[[, T5S, 00:00:14...|[[When,,, 4.1], [...|\n",
      "|     true|  5|[[, T6S, 00:00:18...|[[The,,, 5.1], [l...|\n",
      "|     true|  6|[[, T6E, 00:00:21...|[[Remove,,, 6.1],...|\n",
      "|     null|  7|[[, T7S, 00:00:21...|[[Bring,,, 7.1], ...|\n",
      "|     null|  8|[[, T7E, 00:00:24...|[[Watch,,, 8.1], ...|\n",
      "|     null|  9|[[, T8S, 00:00:27...|[[Police,,, 9.1],...|\n",
      "|     null| 10|[[, T9S, 00:00:29...|[[Bring,,, 10.1],...|\n",
      "|     null| 11|[[, T10S, 00:00:3...|[[You,,, 11.1], [...|\n",
      "|     null| 12|[[, T12S, 00:00:4...|[[-,,, 12.1], [De...|\n",
      "|     null| 13|[[, T12E, 00:00:4...|[[-,,, 13.1], [Ye...|\n",
      "|     null| 14|[[, T13S, 00:00:4...|[[-,,, 14.1], [Fr...|\n",
      "|     null| 15|[[, T13E, 00:00:4...|[[-,,, 15.1], [Ye...|\n",
      "|     null| 16|[[, T14S, 00:00:4...|[[You,,, 16.1], [...|\n",
      "|     null| 17|[[, T15S, 00:00:5...|[[Yeah,,, 17.1], ...|\n",
      "|     null| 18|[[, T16S, 00:00:5...|[[Watch,,, 18.1],...|\n",
      "|     null| 19|[[, T17S, 00:00:5...|[[We,,, 19.1], [g...|\n",
      "|     null| 20|[[, T17E, 00:00:5...|[[How,,, 20.1], [...|\n",
      "+---------+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample_film = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "                                .options(rowTag='s') \\\n",
    "                                .load('sample_dataset/2017/6464116/6887453.xml.gz')\n",
    "imdb_id = '6464116'\n",
    "df_sample_film.printSchema()\n",
    "df_sample_film.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the schema and the first 20 entries of the dataframe containing the subtitles. We see that it contains a lot of null values and information we want to get rid of. Each word array contains an Id we don't need and per row entry we have an array of arrays with words and the times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider storing the sentences in a list of words as it seems to be the best way to perform queries such as counting the number of distinct words or counting the common words between films. We create a second function `udf_sentence` to generate the sentence as a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentence(words):\n",
    "    \"\"\"Function to map the struct containing the words \n",
    "    to a list of words \"\"\"\n",
    "    w_list = []\n",
    "    for w in words:\n",
    "        if w['_VALUE']:\n",
    "            w_list.append(w['_VALUE'])\n",
    "    return w_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to spark function\n",
    "udf_word_array = udf(to_sentence, ArrayType(StringType()))\n",
    "# Define function to create sentence \n",
    "udf_sentence = udf(lambda x: ' '.join([w[0] for w in x]), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentences (list of words in order)\n",
    "df_sample_film_sentence_list = df_sample_film.withColumn(\"sentence\", udf_word_array(\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------------+--------------------+--------------------+\n",
      "|_emphasis|_id|                time|                   w|            sentence|\n",
      "+---------+---+--------------------+--------------------+--------------------+\n",
      "|     null|  1|[[, T1S, 00:00:01...|[[[,,, 1.1], [sho...|[[, shots, firing...|\n",
      "|     null|  2|[[, T2S, 00:00:09...|[[[,,, 2.1], [buz...|[[, buzzer, ], [,...|\n",
      "|     true|  3|[[, T4S, 00:00:12...|[[Clear,,, 3.1], ...|[Clear, and, hols...|\n",
      "|     null|  4|[[, T5S, 00:00:14...|[[When,,, 4.1], [...|[When, your, weap...|\n",
      "|     true|  5|[[, T6S, 00:00:18...|[[The,,, 5.1], [l...|[The, line, is, s...|\n",
      "|     true|  6|[[, T6E, 00:00:21...|[[Remove,,, 6.1],...|[Remove, your, ey...|\n",
      "|     null|  7|[[, T7S, 00:00:21...|[[Bring,,, 7.1], ...|[Bring, your, tar...|\n",
      "|     null|  8|[[, T7E, 00:00:24...|[[Watch,,, 8.1], ...|[Watch, your, hea...|\n",
      "|     null|  9|[[, T8S, 00:00:27...|[[Police,,, 9.1],...|[Police, your, br...|\n",
      "|     null| 10|[[, T9S, 00:00:29...|[[Bring,,, 10.1],...|[Bring, your, tar...|\n",
      "|     null| 11|[[, T10S, 00:00:3...|[[You,,, 11.1], [...|[You, can, frame,...|\n",
      "|     null| 12|[[, T12S, 00:00:4...|[[-,,, 12.1], [De...|[-, Detective, Li...|\n",
      "|     null| 13|[[, T12E, 00:00:4...|[[-,,, 13.1], [Ye...|        [-, Yeah, .]|\n",
      "|     null| 14|[[, T13S, 00:00:4...|[[-,,, 14.1], [Fr...|[-, From, Intelli...|\n",
      "|     null| 15|[[, T13E, 00:00:4...|[[-,,, 15.1], [Ye...|        [-, Yeah, .]|\n",
      "|     null| 16|[[, T14S, 00:00:4...|[[You,,, 16.1], [...|[You, got, a, new...|\n",
      "|     null| 17|[[, T15S, 00:00:5...|[[Yeah,,, 17.1], ...|           [Yeah, .]|\n",
      "|     null| 18|[[, T16S, 00:00:5...|[[Watch,,, 18.1],...|[Watch, your, bac...|\n",
      "|     null| 19|[[, T17S, 00:00:5...|[[We,,, 19.1], [g...|[We, got, to, rol...|\n",
      "|     null| 20|[[, T17E, 00:00:5...|[[How,,, 20.1], [...|[How, 'd, you, do...|\n",
      "+---------+---+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample_film_sentence_list.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------------+--------------------+--------------------+\n",
      "|_emphasis|_id|                time|                   w|            sentence|\n",
      "+---------+---+--------------------+--------------------+--------------------+\n",
      "|     null|  1|[[, T1S, 00:00:01...|[[[,,, 1.1], [sho...|    [ shots firing ]|\n",
      "|     null|  2|[[, T2S, 00:00:09...|[[[,,, 2.1], [buz...|[ buzzer ] [ casi...|\n",
      "|     true|  3|[[, T4S, 00:00:12...|[[Clear,,, 3.1], ...|Clear and holster...|\n",
      "|     null|  4|[[, T5S, 00:00:14...|[[When,,, 4.1], [...|When your weapon ...|\n",
      "|     true|  5|[[, T6S, 00:00:18...|[[The,,, 5.1], [l...|  The line is safe .|\n",
      "|     true|  6|[[, T6E, 00:00:21...|[[Remove,,, 6.1],...|Remove your eyes ...|\n",
      "|     null|  7|[[, T7S, 00:00:21...|[[Bring,,, 7.1], ...|Bring your target...|\n",
      "|     null|  8|[[, T7E, 00:00:24...|[[Watch,,, 8.1], ...|  Watch your heads .|\n",
      "|     null|  9|[[, T8S, 00:00:27...|[[Police,,, 9.1],...| Police your brass .|\n",
      "|     null| 10|[[, T9S, 00:00:29...|[[Bring,,, 10.1],...|Bring your target...|\n",
      "|     null| 11|[[, T10S, 00:00:3...|[[You,,, 11.1], [...|You can frame thi...|\n",
      "|     null| 12|[[, T12S, 00:00:4...|[[-,,, 12.1], [De...|- Detective Linds...|\n",
      "|     null| 13|[[, T12E, 00:00:4...|[[-,,, 13.1], [Ye...|            - Yeah .|\n",
      "|     null| 14|[[, T13S, 00:00:4...|[[-,,, 14.1], [Fr...|- From Intelligen...|\n",
      "|     null| 15|[[, T13E, 00:00:4...|[[-,,, 15.1], [Ye...|            - Yeah .|\n",
      "|     null| 16|[[, T14S, 00:00:4...|[[You,,, 16.1], [...|You got a new guy...|\n",
      "|     null| 17|[[, T15S, 00:00:5...|[[Yeah,,, 17.1], ...|              Yeah .|\n",
      "|     null| 18|[[, T16S, 00:00:5...|[[Watch,,, 18.1],...|   Watch your back .|\n",
      "|     null| 19|[[, T17S, 00:00:5...|[[We,,, 19.1], [g...|    We got to roll .|\n",
      "|     null| 20|[[, T17E, 00:00:5...|[[How,,, 20.1], [...|     How 'd you do ?|\n",
      "+---------+---+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample_film_sentence_string = df_sample_film.withColumn(\"sentence\", udf_sentence(\"w\"))\n",
    "df_sample_film_sentence_string.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode sentences to words for word counts\n",
    "df_sample_film_words = df_sample_film_sentence_list.select('*', explode(col(\"sentence\")).alias('word'))\n",
    "# Filter strings that are not words\n",
    "df_sample_film_words = df_sample_film_words.filter(df_sample_film_words.word.rlike(\"^[a-zA-Z]+$\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct words in film is: 1341\n",
      "Total number of  words in film is: 5454\n"
     ]
    }
   ],
   "source": [
    "word_count_distinct = df_sample_film_words.select(\"word\").distinct().count()\n",
    "word_count_total = df_sample_film_words.select(\"word\").count()\n",
    "\n",
    "print(\"Number of distinct words in film is: {:}\".format(word_count_distinct))\n",
    "print(\"Total number of  words in film is: {:}\".format(word_count_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def film_words(df_film):\n",
    "    \"\"\"Function that returns proper dataframe and word statistics\"\"\"\n",
    "    df_words = df_film.withColumn(\"sentence\", udf_word_array(\"w\")) \\\n",
    "                      .select('*', explode(col(\"sentence\")).alias('word'))\n",
    "    \n",
    "    #TODO change udf_sentence to filter out empty strings and marks.\n",
    "    \n",
    "    # Filter punctuation characters \n",
    "    df_words_filter = df_words.filter(df_words.word.rlike(\"^[a-zA-Z]+$\"))\n",
    "    word_count_distinct = df_words_filter.select(\"word\").distinct().count()\n",
    "    word_count_total = df_words_filter.select(\"word\").count()\n",
    "    \n",
    "    return df_words_filter, word_count_distinct, word_count_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtitle_df(df_film, identifier):\n",
    "    \"\"\"Function that returns proper dataframe and word statistics\"\"\"\n",
    "    df_words, word_count, total_words = film_words(df_film)\n",
    "    \n",
    "    df_result = df_words.withColumn(\"imdb_id\", lit(identifier))\\\n",
    "                        .selectExpr(\"imdb_id\", \"_id as sentence_id\", \"word\")\n",
    "    \n",
    "    return df_result, word_count, word_count_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct words : 1341\n",
      "Number of words : 5454\n",
      "+-------+-----------+-------+\n",
      "|imdb_id|sentence_id|   word|\n",
      "+-------+-----------+-------+\n",
      "|6464116|          1|  shots|\n",
      "|6464116|          1| firing|\n",
      "|6464116|          2| buzzer|\n",
      "|6464116|          2|casings|\n",
      "|6464116|          2|clatter|\n",
      "|6464116|          3|  Clear|\n",
      "|6464116|          3|    and|\n",
      "|6464116|          3|holster|\n",
      "|6464116|          3|   your|\n",
      "|6464116|          3|   safe|\n",
      "|6464116|          3| weapon|\n",
      "|6464116|          4|   When|\n",
      "|6464116|          4|   your|\n",
      "|6464116|          4| weapon|\n",
      "|6464116|          4|     is|\n",
      "|6464116|          4|   safe|\n",
      "|6464116|          4|  raise|\n",
      "|6464116|          4|   your|\n",
      "|6464116|          4|   hand|\n",
      "|6464116|          5|    The|\n",
      "+-------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "film, words_distinct, words_total = subtitle_df(df_sample_film, imdb_id)\n",
    "\n",
    "print('Number of distinct words : {}'.format(words_distinct))\n",
    "print('Number of words : {}'.format(words_total))\n",
    "film.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for clean processing\n",
    "#TODO consider if \"explode\" beforehand or not\n",
    "def subtitle_cleaning(df_film, identifier):\n",
    "    df_words = df_film.withColumn(\"sentence\", udf_word_array(\"w\")) \\\n",
    "                      .select('*', explode(col(\"sentence\")).alias('word'))\n",
    "    df_words_filter = df_words.filter(df_words.word.rlike(\"^[a-zA-Z]+$\"))\n",
    "    df_result = df_words_filter.withColumn(\"imdb_id\", lit(identifier)).select(\"imdb_id\", \"_id\", \"word\")\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function which will give us a resulting metadata dataframe with the information we want. We separate genres as an array of strings for later queries. We also associate an IMDb Id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO check if convert genres to lowercase?\n",
    "#TODO what to do if missing columns: source duration, need to choose\n",
    "\n",
    "udf_split = udf(str.split, ArrayType(StringType()))\n",
    "\n",
    "def meta_data_cleaning(df_metadata, identifier):\n",
    "    df = df_metadata.withColumn(\"imdb_id\", lit(identifier)).selectExpr(\"imdb_id\", \n",
    "                                                                       \"conversion.sentences\",\n",
    "                                                                       \"source.genre\", \n",
    "                                                                       \"source.year\", \n",
    "                                                                       \"subtitle.blocks\",\n",
    "                                                                       \"subtitle.duration as subtitle_duration\",\n",
    "                                                                       \"subtitle.language\")\n",
    "    df = df.withColumn(\"genres\", udf_split(\"genre\")).drop(\"genre\")\n",
    "    return df\n",
    "\n",
    "df_md = meta_data_cleaning(df_sample_metadata, imdb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----+------+-----------------+--------+--------------------+\n",
      "|imdb_id|sentences|year|blocks|subtitle_duration|language|              genres|\n",
      "+-------+---------+----+------+-----------------+--------+--------------------+\n",
      "|6464116|      967|2017|   888|     00:40:42,361| English|[Action,Crime,Drama]|\n",
      "+-------+---------+----+------+-----------------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_md.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing a simple XML file, we code a program to make a resulting dataframe where we can make different queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_maker(path):\n",
    "    df_md = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "                                    .options(rowTag='meta') \\\n",
    "                                    .load(path)\n",
    "    df_sub = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "                                    .options(rowTag='s') \\\n",
    "                                    .load(path)\n",
    "    return (df_md, df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ac00b2d4fcd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mcurrent_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0myear\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimdb_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mdf_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_sub\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mdf_metadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_metadatas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_data_cleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimdb_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mdf_subtitles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_subtitles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtitle_cleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimdb_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-ecd2e0f7858b>\u001b[0m in \u001b[0;36mdataframe_maker\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdataframe_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdf_md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'com.databricks.spark.xml'\u001b[0m\u001b[0;34m)\u001b[0m                                    \u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrowTag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'meta'\u001b[0m\u001b[0;34m)\u001b[0m                                     \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'com.databricks.spark.xml'\u001b[0m\u001b[0;34m)\u001b[0m                                    \u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrowTag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m)\u001b[0m                                     \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_sub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = \"sample_dataset/\"\n",
    "df_metadatas = spark.createDataFrame([], df_md.schema)\n",
    "df_subtitles = spark.createDataFrame([], film.schema)\n",
    "for year in os.listdir(path):\n",
    "    for imdb_id in os.listdir(path + year):\n",
    "        current_path = path + year + \"/\" + imdb_id\n",
    "        for file in os.listdir(current_path):\n",
    "            (df_m, df_sub) = dataframe_maker(current_path + '/' + file)\n",
    "            df_metadatas = df_metadatas.union(meta_data_cleaning(df_m, imdb_id))\n",
    "            df_subtitles = df_subtitles.union(subtitle_cleaning(df_sub, imdb_id))\n",
    "#             df_m.show()\n",
    "#             print(current_path + \"/\" + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a metadata and subtitle dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadatas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtitles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtitles = df_subtitles.groupBy(\"imdb_id\").count()\n",
    "df_subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "df_md.join(film, [\"imdb_id\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection (updated README too)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_That you considered ways to enrich, filter, transform the data according to your needs._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to locally download the IMDb dataset and load it into a dataframe but we would like to scrape the datasets. On top of this, we would also like to scrape some more data from the IMDb website not present in the IMDb datasets: box office and reviewers' comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_That you have updated your plan in a reasonable way, reflecting your improved knowledge after data acquaintance. In particular, discuss how your data suits your project needs and discuss the methods you’re going to use, giving their essential mathematical details in the notebook._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily compute word statistics (such as mean length of words, number of distinct words & total number of words) given that we successfully transformed the XML files into spark dataframes. That said, we still can not figure out why `df.groupBy(\"imdb_id\").count()` does not work for us, and we will tackle this problem before continuing.\n",
    "\n",
    "We will define sets of words for certain themes (e.g. love, hate, sex, drugs, war, slang...) and see how they correlate with the movie's popularity (IMDb rating, number of votes and popularity). In order to do that, we will define functions to query the text. We might create \"super strings\" that contain the whole subtitles of a movie and use those to see how many of words defined in the sets appear in the scripts.\n",
    "\n",
    "We are also considering to use the SpaCy library in order to give a score to the similarity of differents scripts. This will help us in defining new categories of films. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_That your plan for analysis and communication is now reasonable and sound, potentially discussing alternatives to your choices that you considered but dropped._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realized that extracting the timestamp from each subtitle block was more complicated than expected. We thought we would find one start time and one end time in each block, but there was actually no consistency whatsoever. \n",
    "\n",
    "For the moment the plan seems ambitious but feasible.\n",
    "\n",
    "The main difficulty will be to determine the features that are meaningful in order to group movies, implement a Machine Learning pipeline for the prediction and scrape IMDb's website for the reviewer's comments and missing figures (box-office)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
