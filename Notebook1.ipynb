{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Movie behind a Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import urllib.request\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.10:0.4.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set('spark.sql.session.timeZone', 'UTC')\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of datasets\n",
    "\n",
    "The OpenSubtitles dataset is a compressed cluster of folders containing XML files. Each XML file is split into a script portion with the subtitles of the movie and a metadata portion with additional information about the movie or show. The name of one of the parent folders of the XML file is the corresponding IMDb identifier of the movie or show, thus allowing us to extract additional information from the IMDb dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDb Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have at our disposal the IMDb ratings and basics dataset. For the moment we have downloaded the files locally, but we would like to scrape the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO scrape data https://datasets.imdbws.com/\n",
    "ratings_fn = \"title.ratings.tsv.gz\"\n",
    "basics_fn = \"title.basics.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------+\n",
      "|   tconst|averageRating|numVotes|\n",
      "+---------+-------------+--------+\n",
      "|tt0000001|          5.8|    1440|\n",
      "|tt0000002|          6.3|     172|\n",
      "|tt0000003|          6.6|    1041|\n",
      "|tt0000004|          6.4|     102|\n",
      "|tt0000005|          6.2|    1735|\n",
      "|tt0000006|          5.5|      91|\n",
      "|tt0000007|          5.5|     579|\n",
      "|tt0000008|          5.6|    1539|\n",
      "|tt0000009|          5.6|      74|\n",
      "|tt0000010|          6.9|    5127|\n",
      "|tt0000011|          5.4|     214|\n",
      "|tt0000012|          7.4|    8599|\n",
      "|tt0000013|          5.7|    1318|\n",
      "|tt0000014|          7.2|    3739|\n",
      "|tt0000015|          6.2|     660|\n",
      "|tt0000016|          5.9|     982|\n",
      "|tt0000017|          4.8|     197|\n",
      "|tt0000018|          5.5|     414|\n",
      "|tt0000019|          6.6|      13|\n",
      "|tt0000020|          5.1|     232|\n",
      "+---------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ratings = spark.read.option(\"header\", \"true\")\\\n",
    "                       .option(\"sep\", \"\\t\")\\\n",
    "                       .csv(\"imdb_data/\" + ratings_fn)\n",
    "df_ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|              genres|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|tt0000001|    short|          Carmencita|          Carmencita|      0|     1894|     \\N|             1|   Documentary,Short|\n",
      "|tt0000002|    short|Le clown et ses c...|Le clown et ses c...|      0|     1892|     \\N|             5|     Animation,Short|\n",
      "|tt0000003|    short|      Pauvre Pierrot|      Pauvre Pierrot|      0|     1892|     \\N|             4|Animation,Comedy,...|\n",
      "|tt0000004|    short|         Un bon bock|         Un bon bock|      0|     1892|     \\N|            \\N|     Animation,Short|\n",
      "|tt0000005|    short|    Blacksmith Scene|    Blacksmith Scene|      0|     1893|     \\N|             1|        Comedy,Short|\n",
      "|tt0000006|    short|   Chinese Opium Den|   Chinese Opium Den|      0|     1894|     \\N|             1|               Short|\n",
      "|tt0000007|    short|Corbett and Court...|Corbett and Court...|      0|     1894|     \\N|             1|         Short,Sport|\n",
      "|tt0000008|    short|Edison Kinetoscop...|Edison Kinetoscop...|      0|     1894|     \\N|             1|   Documentary,Short|\n",
      "|tt0000009|    movie|          Miss Jerry|          Miss Jerry|      0|     1894|     \\N|            45|             Romance|\n",
      "|tt0000010|    short|Employees Leaving...|La sortie de l'us...|      0|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000011|    short|Akrobatisches Pot...|Akrobatisches Pot...|      0|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000012|    short|The Arrival of a ...|L'arrivée d'un tr...|      0|     1896|     \\N|             1|   Documentary,Short|\n",
      "|tt0000013|    short|The Photographica...|Neuville-sur-Saôn...|      0|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000014|    short|Tables Turned on ...|   L'arroseur arrosé|      0|     1895|     \\N|             1|        Comedy,Short|\n",
      "|tt0000015|    short| Autour d'une cabine| Autour d'une cabine|      0|     1894|     \\N|             2|     Animation,Short|\n",
      "|tt0000016|    short|Barque sortant du...|Barque sortant du...|      0|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000017|    short|Italienischer Bau...|Italienischer Bau...|      0|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000018|    short|Das boxende Känguruh|Das boxende Känguruh|      0|     1895|     \\N|             1|               Short|\n",
      "|tt0000019|    short|    The Clown Barber|    The Clown Barber|      0|     1898|     \\N|            \\N|        Comedy,Short|\n",
      "|tt0000020|    short|      The Derby 1895|      The Derby 1895|      0|     1895|     \\N|             1|Documentary,Short...|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_basics = spark.read.option(\"header\", \"true\")\\\n",
    "                      .option(\"sep\", \"\\t\")\\\n",
    "                      .csv(\"imdb_data/\" + basics_fn)\n",
    "df_basics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenSubtitles dataset\n",
    "\n",
    "The dataset consists of 31 GB of XML files distributed in the following file structure: \n",
    "\n",
    "```\n",
    "├── opensubtitle\n",
    "│   ├── OpenSubtitles2018\n",
    "│   │   ├── Year\n",
    "│   │   │   ├── Id\n",
    "│   │   │   │   ├── #######.xml.gz\n",
    "│   │   │   │   ├── #######.xml.gz\n",
    "│   ├── en.tar.gz\n",
    "│   ├── fr.tar.gz\n",
    "│   ├── zh_cn.tar.gz\n",
    "```\n",
    "where\n",
    "- `######` is a 6-digit unique identifier of the file on the OpenSubtitles dataset.\n",
    "- `Year` is the year the movie or episode was made.\n",
    "- `Id` is a 5 to 7 digit identifier (if it's 7-digit it's also an IMDb identifier).\n",
    "\n",
    "The subtitles are provided in different languages. We only analyze the `OpenSubtitles2018` folder and it's the only folder we detail.\n",
    "\n",
    "The decompressed XML files vary in size, ranging from 5KB to 9000KB sized files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML Files\n",
    "\n",
    "Each XML file is split into a `document` and `metadata` section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtitles\n",
    "\n",
    "The `document` section contains all the subtitles and its general structure is the following:\n",
    "\n",
    "```\n",
    "├── s\n",
    "│   ├── time: Integer\n",
    "│   ├── w: String\n",
    "```\n",
    "\n",
    "An example snippet of an XML file:\n",
    "\n",
    "```xml\n",
    "  <s id=\"1\">\n",
    "    <time id=\"T1S\" value=\"00:00:51,819\" />\n",
    "    <w id=\"1.1\">Travis</w>\n",
    "    <w id=\"1.2\">.</w>\n",
    "    <time id=\"T1E\" value=\"00:00:53,352\" />\n",
    "  </s>\n",
    "```\n",
    "\n",
    "The subtitles in each XML file are stored by blocks denoted by `s` with a unique `id` attribute (integers in increasing order starting at 1).  \n",
    "\n",
    "Each block (`<s id=\"1\">` for instance) has a:  \n",
    "\n",
    "1. Set of timestamps (denoted by `time`) with\n",
    " - A timestamp `id` attribute that can take two different formats: `T#S` or `T#E`, where _S_ indicates _start_, _E_ indicates _end_ and _#_ is an increasing integer. \n",
    " - A `value` attribute which has the format `HH:mm:ss,fff`.\n",
    "\n",
    "2. Set of words (denoted by `w`) with\n",
    " - an `id` attribute that is simply an increasing number of decimal numbers of the format `X.Y` where X is the string id and Y is the word id within the corresponding string\n",
    " - a non-empty `value` attribute that contains a token: a word or a punctuation character. \n",
    "\n",
    "It sometimes also has an `alternative`, `initial` and `emphasis` attribute.  \n",
    "\n",
    " - The `initial` attribute generally corresponds to slang words or mispronounced words because of an accent such as _lyin'_ instead of _lying_.  \n",
    " - The `alternative` attribute is another way of displaying the subtitle for example _HOW_ instead of _how_.\n",
    " - The `emphasis` attribute is a boolean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "The `metadata` section has the following structure:\n",
    "\n",
    "```\n",
    "├── Conversion\n",
    "│   ├── corrected_words: Integer\n",
    "│   ├── sentences: Integer\n",
    "│   ├── tokens: Integer\n",
    "│   ├── encoding: String (always utf-8)\n",
    "│   ├── unknown_words: Integer\n",
    "│   ├── ignored_blocks: Integer\n",
    "│   ├── truecased_words: Integer\n",
    "├── Subtitle\n",
    "│   ├── language: String\n",
    "│   ├── date: String\n",
    "│   ├── duration: String\n",
    "│   ├── cds: String (presented as #/# where # is an int)\n",
    "│   ├── blocks: Integer\n",
    "│   ├── confidence: Double\n",
    "├── Source\n",
    "│   ├── genre: String[] (up to 3 genres)\n",
    "│   ├── year: Integer\n",
    "│   ├── duration: Integer (in minutes)\n",
    "│   ├── original: String\n",
    "│   ├── country: String\n",
    "```\n",
    "\n",
    "We note that some XML files may not have all the entries. \n",
    "We can use the metadata to obtain additional information about the movie or show's subtitles and compute certain statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- conversion: struct (nullable = true)\n",
      " |    |-- corrected_words: long (nullable = true)\n",
      " |    |-- encoding: string (nullable = true)\n",
      " |    |-- ignored_blocks: long (nullable = true)\n",
      " |    |-- sentences: long (nullable = true)\n",
      " |    |-- tokens: long (nullable = true)\n",
      " |    |-- truecased_words: long (nullable = true)\n",
      " |    |-- unknown_words: long (nullable = true)\n",
      " |-- source: struct (nullable = true)\n",
      " |    |-- duration: long (nullable = true)\n",
      " |    |-- genre: string (nullable = true)\n",
      " |    |-- year: long (nullable = true)\n",
      " |-- subtitle: struct (nullable = true)\n",
      " |    |-- blocks: long (nullable = true)\n",
      " |    |-- cds: string (nullable = true)\n",
      " |    |-- confidence: double (nullable = true)\n",
      " |    |-- date: string (nullable = true)\n",
      " |    |-- duration: string (nullable = true)\n",
      " |    |-- language: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|          conversion|              source|            subtitle|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[0, utf-8, 2, 967...|[41, Action,Crime...|[888, 1/1, 1.0, 2...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample_metadata = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "                                    .options(rowTag='meta') \\\n",
    "                                    .load('sample_dataset/2017/6464116/6887453.xml.gz')\n",
    "\n",
    "df_sample_metadata.printSchema()\n",
    "df_sample_metadata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the schema. We need to decide what is actually relevant for us to filter out the useless information and choose which format our dataframe should have (for example having all the different genres in a separate column.)\n",
    "\n",
    "For the metadata we have a very clean dataframe which can be used for a lot of statistics and filtering : filtering by genre, by year etc. \n",
    "\n",
    "We can see that there is no actual link between our dataframes (subtitles and metadata) : neither contain the movie id that would pair them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "Going through the dataset we notice a few things:\n",
    "\n",
    "1. The dataset has meaningless folders. For example, the folder 1858/ is empty.\n",
    "2. Dataset contains XML files that are not related to movies or TV shows. For example, the folder 666/ contains Justin Bieber song subtitles.  \n",
    "3. Trailer of films can be present in the dataset. For example, the folder 2018/ we found for example Black Panther teaser trailer subtitles.\n",
    "4. Each movie might have more than 1 subtitle file.\n",
    "5. Some subtitle files contain text that is not related to the movie, like credits to the person who made the subtitles.\n",
    "6. The IDMDb folder name is not always a 7-digit number, meaning it is not always a valid IMDb identifer and we can't retrieve the IMDb info.\n",
    "7. Each block may have an arbitrary number (including 0) of timestamps associated to it.\n",
    "\n",
    "To solve points 1 and 2, we ignore all the folders which aren't inside the range of 1920-2018.\n",
    "\n",
    "To solve point 3, we drop trailers by looking at the `duration` field in the metadata section.\n",
    "\n",
    "To solve point 4, we simply take the first one.\n",
    "\n",
    "To solve point 6, we keep movies that have a correct IMDb identifier. Hence, all the files in folders that don't have a 7-digit folder name are dropped.\n",
    "\n",
    "To solve point 7, we decide not to associate a timestamp to each word for the moment.\n",
    " \n",
    "For the moment, we take a sample of the dataset from the cluster (see python script `extract_sample_2.py`) by collecting 1 or 2 movies for each year in the range 1920-2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and structuring data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtitles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _emphasis: boolean (nullable = true)\n",
      " |-- _id: long (nullable = true)\n",
      " |-- time: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _id: string (nullable = true)\n",
      " |    |    |-- _value: string (nullable = true)\n",
      " |-- w: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _alternative: string (nullable = true)\n",
      " |    |    |-- _emphasis: boolean (nullable = true)\n",
      " |    |    |-- _id: double (nullable = true)\n",
      "\n",
      "+---------+---+--------------------+--------------------+\n",
      "|_emphasis|_id|                time|                   w|\n",
      "+---------+---+--------------------+--------------------+\n",
      "|     null|  1|[[, T1S, 00:00:01...|[[[,,, 1.1], [sho...|\n",
      "|     null|  2|[[, T2S, 00:00:09...|[[[,,, 2.1], [buz...|\n",
      "|     true|  3|[[, T4S, 00:00:12...|[[Clear,,, 3.1], ...|\n",
      "|     null|  4|[[, T5S, 00:00:14...|[[When,,, 4.1], [...|\n",
      "|     true|  5|[[, T6S, 00:00:18...|[[The,,, 5.1], [l...|\n",
      "|     true|  6|[[, T6E, 00:00:21...|[[Remove,,, 6.1],...|\n",
      "|     null|  7|[[, T7S, 00:00:21...|[[Bring,,, 7.1], ...|\n",
      "|     null|  8|[[, T7E, 00:00:24...|[[Watch,,, 8.1], ...|\n",
      "|     null|  9|[[, T8S, 00:00:27...|[[Police,,, 9.1],...|\n",
      "|     null| 10|[[, T9S, 00:00:29...|[[Bring,,, 10.1],...|\n",
      "|     null| 11|[[, T10S, 00:00:3...|[[You,,, 11.1], [...|\n",
      "|     null| 12|[[, T12S, 00:00:4...|[[-,,, 12.1], [De...|\n",
      "|     null| 13|[[, T12E, 00:00:4...|[[-,,, 13.1], [Ye...|\n",
      "|     null| 14|[[, T13S, 00:00:4...|[[-,,, 14.1], [Fr...|\n",
      "|     null| 15|[[, T13E, 00:00:4...|[[-,,, 15.1], [Ye...|\n",
      "|     null| 16|[[, T14S, 00:00:4...|[[You,,, 16.1], [...|\n",
      "|     null| 17|[[, T15S, 00:00:5...|[[Yeah,,, 17.1], ...|\n",
      "|     null| 18|[[, T16S, 00:00:5...|[[Watch,,, 18.1],...|\n",
      "|     null| 19|[[, T17S, 00:00:5...|[[We,,, 19.1], [g...|\n",
      "|     null| 20|[[, T17E, 00:00:5...|[[How,,, 20.1], [...|\n",
      "+---------+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample_film = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "                                .options(rowTag='s') \\\n",
    "                                .load('sample_dataset/2017/6464116/6887453.xml.gz')\n",
    "df_sample_film.printSchema()\n",
    "df_sample_film.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the schema and the first 20 entries of the dataframe containing the subtitles. We see that it contains a lot of null values and information we want to get rid of. Each word array contains an Id we don't need and per row entry we have an array of arrays with words and the times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider storing the sentences in a list of words as it seems to be the best way to perform queries such as counting the number of distinct words or counting the common words between films. We create a second function `udf_sentence` to generate the sentence as a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentence(words):\n",
    "    \"\"\"Function to map the struct containing the words \n",
    "    to a list of words \"\"\"\n",
    "    w_list = []\n",
    "    for w in words:\n",
    "        if not w['_VALUE']:\n",
    "            w_list.append(w['_VALUE'])\n",
    "    return w_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to spark function\n",
    "udf_word_array = udf(to_sentence, ArrayType(StringType()))\n",
    "# Define function to create sentence \n",
    "udf_sentence = udf(lambda x: ' '.join([w[0] for w in x]), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentences (list of words in order)\n",
    "df_sample_film_sentence_list = df_sample_film.withColumn(\"sentence\", udf_word_array(\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------------+--------------------+--------+\n",
      "|_emphasis|_id|                time|                   w|sentence|\n",
      "+---------+---+--------------------+--------------------+--------+\n",
      "|     null|  1|[[, T1S, 00:00:01...|[[[,,, 1.1], [sho...|      []|\n",
      "|     null|  2|[[, T2S, 00:00:09...|[[[,,, 2.1], [buz...|      []|\n",
      "|     true|  3|[[, T4S, 00:00:12...|[[Clear,,, 3.1], ...|      []|\n",
      "|     null|  4|[[, T5S, 00:00:14...|[[When,,, 4.1], [...|      []|\n",
      "|     true|  5|[[, T6S, 00:00:18...|[[The,,, 5.1], [l...|      []|\n",
      "|     true|  6|[[, T6E, 00:00:21...|[[Remove,,, 6.1],...|      []|\n",
      "|     null|  7|[[, T7S, 00:00:21...|[[Bring,,, 7.1], ...|      []|\n",
      "|     null|  8|[[, T7E, 00:00:24...|[[Watch,,, 8.1], ...|      []|\n",
      "|     null|  9|[[, T8S, 00:00:27...|[[Police,,, 9.1],...|      []|\n",
      "|     null| 10|[[, T9S, 00:00:29...|[[Bring,,, 10.1],...|      []|\n",
      "|     null| 11|[[, T10S, 00:00:3...|[[You,,, 11.1], [...|      []|\n",
      "|     null| 12|[[, T12S, 00:00:4...|[[-,,, 12.1], [De...|      []|\n",
      "|     null| 13|[[, T12E, 00:00:4...|[[-,,, 13.1], [Ye...|      []|\n",
      "|     null| 14|[[, T13S, 00:00:4...|[[-,,, 14.1], [Fr...|      []|\n",
      "|     null| 15|[[, T13E, 00:00:4...|[[-,,, 15.1], [Ye...|      []|\n",
      "|     null| 16|[[, T14S, 00:00:4...|[[You,,, 16.1], [...|      []|\n",
      "|     null| 17|[[, T15S, 00:00:5...|[[Yeah,,, 17.1], ...|      []|\n",
      "|     null| 18|[[, T16S, 00:00:5...|[[Watch,,, 18.1],...|      []|\n",
      "|     null| 19|[[, T17S, 00:00:5...|[[We,,, 19.1], [g...|      []|\n",
      "|     null| 20|[[, T17E, 00:00:5...|[[How,,, 20.1], [...|      []|\n",
      "+---------+---+--------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample_film_sentence_list.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------------+--------------------+--------------------+\n",
      "|_emphasis|_id|                time|                   w|            sentence|\n",
      "+---------+---+--------------------+--------------------+--------------------+\n",
      "|     null|  1|[[, T1S, 00:00:01...|[[[,,, 1.1], [sho...|    [ shots firing ]|\n",
      "|     null|  2|[[, T2S, 00:00:09...|[[[,,, 2.1], [buz...|[ buzzer ] [ casi...|\n",
      "|     true|  3|[[, T4S, 00:00:12...|[[Clear,,, 3.1], ...|Clear and holster...|\n",
      "|     null|  4|[[, T5S, 00:00:14...|[[When,,, 4.1], [...|When your weapon ...|\n",
      "|     true|  5|[[, T6S, 00:00:18...|[[The,,, 5.1], [l...|  The line is safe .|\n",
      "|     true|  6|[[, T6E, 00:00:21...|[[Remove,,, 6.1],...|Remove your eyes ...|\n",
      "|     null|  7|[[, T7S, 00:00:21...|[[Bring,,, 7.1], ...|Bring your target...|\n",
      "|     null|  8|[[, T7E, 00:00:24...|[[Watch,,, 8.1], ...|  Watch your heads .|\n",
      "|     null|  9|[[, T8S, 00:00:27...|[[Police,,, 9.1],...| Police your brass .|\n",
      "|     null| 10|[[, T9S, 00:00:29...|[[Bring,,, 10.1],...|Bring your target...|\n",
      "|     null| 11|[[, T10S, 00:00:3...|[[You,,, 11.1], [...|You can frame thi...|\n",
      "|     null| 12|[[, T12S, 00:00:4...|[[-,,, 12.1], [De...|- Detective Linds...|\n",
      "|     null| 13|[[, T12E, 00:00:4...|[[-,,, 13.1], [Ye...|            - Yeah .|\n",
      "|     null| 14|[[, T13S, 00:00:4...|[[-,,, 14.1], [Fr...|- From Intelligen...|\n",
      "|     null| 15|[[, T13E, 00:00:4...|[[-,,, 15.1], [Ye...|            - Yeah .|\n",
      "|     null| 16|[[, T14S, 00:00:4...|[[You,,, 16.1], [...|You got a new guy...|\n",
      "|     null| 17|[[, T15S, 00:00:5...|[[Yeah,,, 17.1], ...|              Yeah .|\n",
      "|     null| 18|[[, T16S, 00:00:5...|[[Watch,,, 18.1],...|   Watch your back .|\n",
      "|     null| 19|[[, T17S, 00:00:5...|[[We,,, 19.1], [g...|    We got to roll .|\n",
      "|     null| 20|[[, T17E, 00:00:5...|[[How,,, 20.1], [...|     How 'd you do ?|\n",
      "+---------+---+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample_film_sentence_string = df_sample_film.withColumn(\"sentence\", udf_sentence(\"w\"))\n",
    "df_sample_film_sentence_string.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode sentences to words for word counts\n",
    "df_sample_film_words = df_sample_film_sentence_list.select('*', explode(col(\"sentence\")).alias('word'))\n",
    "# Filter strings that are not words\n",
    "df_sample_film_words = df_sample_film_words.filter(df_sample_film_words.word.rlike(\"^[a-zA-Z]+$\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct words in film is: 0\n",
      "Total number of  words in film is: 0\n"
     ]
    }
   ],
   "source": [
    "word_count_distinct = df_sample_film_words.select(\"word\").distinct().count()\n",
    "word_count_total = df_sample_film_words.select(\"word\").count()\n",
    "\n",
    "print(\"Number of distinct words in film is: {:}\".format(word_count_distinct))\n",
    "print(\"Total number of  words in film is: {:}\".format(word_count_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of words in film\n",
    "def film_words(df_film):\n",
    "    df_words = df_film.withColumn(\"sentence\", udf_word_array(\"w\")) \\\n",
    "                      .select('*', explode(col(\"sentence\")).alias('word'))\n",
    "    #TODO change udf_sentence to filter out empty strings and marks.\n",
    "    df_words_filter = df_words.filter(df_words.word.rlike(\"^[a-zA-Z]+$\"))\n",
    "    word_count_distinct = df_words_filter.select(\"word\").distinct().count()\n",
    "    word_count_total = df_words_filter.select(\"word\").count()\n",
    "    return (df_words_filter, word_count_distinct, word_count_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returning proper dataframe and also word statistic\n",
    "def subtitle_df(df_film, identifier):\n",
    "    (df_words, word_count, total_words) = film_words(df_film)\n",
    "    df_result = df_words.withColumn(\"imdb_id\", lit(identifier)).selectExpr(\"imdb_id\", \"_id as sentence_id\", \"word\")\n",
    "    return (df_result, word_count, word_count_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for clean processing\n",
    "#TODO consider if \"explode\" beforehand or not\n",
    "def subtitle_cleaning(df_film, identifier):\n",
    "    df_words = df_film.withColumn(\"sentence\", udf_word_array(\"w\")) \\\n",
    "                      .select('*', explode(col(\"sentence\")).alias('word'))\n",
    "    df_words_filter = df_words.filter(df_words.word.rlike(\"^[a-zA-Z]+$\"))\n",
    "    df_result = df_words_filter.withColumn(\"imdb_id\", lit(identifier)).select(\"imdb_id\", \"_id\", \"word\")\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function which will give us a resulting metadata dataframe with the information we want. We separate genres as an array of strings for later queries. We also associate an IMDb Id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO check if convert genres to lowercase?\n",
    "#TODO what to do if missing columns: source duration, need to choose\n",
    "imdb_id = \"5052448\"\n",
    "udf_split = udf(str.split, ArrayType(StringType()))\n",
    "\n",
    "def meta_data_cleaning(df_metadata, identifier):\n",
    "    df = df_metadata.withColumn(\"imdb_id\", lit(identifier)).selectExpr(\"imdb_id\", \n",
    "                                                                       \"conversion.sentences\",\n",
    "                                                                       \"source.genre\", \n",
    "                                                                       \"source.year\", \n",
    "                                                                       \"subtitle.blocks\",\n",
    "                                                                       \"subtitle.duration as subtitle_duration\",\n",
    "                                                                       \"subtitle.language\")\n",
    "    df = df.withColumn(\"genres\", udf_split(\"genre\")).drop(\"genre\")\n",
    "    return df\n",
    "\n",
    "df_md = meta_data_cleaning(df_sample_metadata, imdb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----+------+-----------------+--------+--------------------+\n",
      "|imdb_id|sentences|year|blocks|subtitle_duration|language|              genres|\n",
      "+-------+---------+----+------+-----------------+--------+--------------------+\n",
      "|5052448|      967|2017|   888|     00:40:42,361| English|[Action,Crime,Drama]|\n",
      "+-------+---------+----+------+-----------------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_md.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing a simple XML file, we code a program to make a resulting dataframe where we can make different queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_maker(path):\n",
    "    df_md = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "                                    .options(rowTag='meta') \\\n",
    "                                    .load(path)\n",
    "    df_sub = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "                                    .options(rowTag='s') \\\n",
    "                                    .load(path)\n",
    "    return (df_md, df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path = \"sample_dataset/\"\n",
    "# df_metadatas = spark.createDataFrame([], df_md.schema)\n",
    "# df_subtitles = spark.createDataFrame([], film.schema)\n",
    "# for year in os.listdir(path):\n",
    "#     for imdb_id in os.listdir(path + year):\n",
    "#         current_path = path + year + \"/\" + imdb_id\n",
    "#         for file in os.listdir(current_path):\n",
    "#             (df_m, df_sub) = dataframe_maker(current_path + '/' + file)\n",
    "#             df_metadatas = df_metadatas.union(meta_data_cleaning(df_m, imdb_id))\n",
    "#             df_subtitles = df_subtitles.union(subtitle_cleaning(df_sub, imdb_id))\n",
    "#             df_m.show()\n",
    "#             print(current_path + \"/\" + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_metadatas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4571dbd8b8e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_metadatas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_metadatas' is not defined"
     ]
    }
   ],
   "source": [
    "df_metadatas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtitles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtitles = df_subtitles.groupBy(\"imdb_id\").count()\n",
    "df_subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film, words_distinct, words_total = subtitle_df(df_sample_film, imdb_id)\n",
    "film.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "df_md.join(film, [\"imdb_id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_distinct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
