{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Movie behind a Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import urllib.request\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.10:0.4.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set('spark.sql.session.timeZone', 'UTC')\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of datasets\n",
    "\n",
    "The OpenSubtitles dataset is a compressed cluster of folders containing XML files. Each XML file is split into a script portion with the subtitles of the movie and a metadata portion with additional information about the movie or show. The name of one of the parent folders of the XML file is the corresponding IMDb identifier of the movie or show, thus allowing us to extract additional information from the IMDb dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenSubtitles dataset\n",
    "\n",
    "The dataset consists of 31 (**TODO: how many?**) GB of XML files distributed in the following file structure: \n",
    "\n",
    "```\n",
    "├── opensubtitle\n",
    "│   ├── OpenSubtitles2018\n",
    "│   │   ├── Year\n",
    "│   │   │   ├── Id\n",
    "│   │   │   │   ├── #######.xml.gz\n",
    "│   │   │   │   ├── #######.xml.gz\n",
    "│   ├── en.tar.gz\n",
    "│   ├── fr.tar.gz\n",
    "│   ├── zh_cn.tar.gz\n",
    "```\n",
    "where\n",
    "- `######` is a 6-digit unique identifier of the file on the OpenSubtitles dataset.\n",
    "- `Year` is the year the movie or episode was made.\n",
    "- `Id` is a 5 to 7 digit identifier (if it's 7-digit it's also an IMDb identifier).\n",
    "\n",
    "The subtitles are provided in different languages. For the moment we only analyze the `OpenSubtitles2018` folder and it's the only folder we detail.\n",
    "\n",
    "Some `Year` folders are not indicative, for instance 0, 666 and 1191. We also notice that for each `Id` we can find multiple subtitle XML files, as illustrated above. The decompressed XML files vary in size, ranging from 5KB to 9000KB sized files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML Files\n",
    "\n",
    "The unique identifier of each XML file is found in the name of the file. Each XML file is split into a `document` and `metadata` section.\n",
    "\n",
    "#### Subtitles\n",
    "\n",
    "The `document` section contains all the subtitles and its general structure is the following:\n",
    "\n",
    "```\n",
    "├── s\n",
    "│   ├── time: Integer\n",
    "│   ├── w: String\n",
    "```\n",
    "\n",
    "An example snippet of an XML file:\n",
    "\n",
    "```xml\n",
    "  <s id=\"1\">\n",
    "    <time id=\"T1S\" value=\"00:00:51,819\" />\n",
    "    <w id=\"1.1\">Travis</w>\n",
    "    <w id=\"1.2\">.</w>\n",
    "    <time id=\"T1E\" value=\"00:00:53,352\" />\n",
    "  </s>\n",
    "```\n",
    "\n",
    "For the actual subtitles in each xml file we can see that they are stored in sentences, each one having an unique id (integers in increasing order starting at 1).  \n",
    "Each sentence (`<s id=\"1\">` for instance) has a:  \n",
    "1. a set of timestamps \n",
    "2. a set of words.\n",
    "\n",
    "Every timestamp and word have also an id and a set of attributes.  \n",
    "The timestamp id can take two different formats: \"T#S\" or \"T#E\", \"S\" indicates \"start\" and \"E\" indicates \"end\",\n",
    "\\# is increasing integer. \n",
    "The words inbetween a start and end of timestamp are shown on the screen from the `value` of __T#S__ to  the `value`of __T#E__. **This is a great indicator of fast dialog !**  \n",
    "Apart from the id, the timestamp also has a `value` attribute which has the format `` HH:mm:ss,fff``.\n",
    "\n",
    "For the words the `id` is simply an increasing number of decimal numbers \"X.Y\" where X is the string id and Y is the word id within the corresponding string. Each word element in the XML file has a non-empty `value` attribute (the actual word, can be a mark) and it might have an `alternative` and `initial` value.  \n",
    "The `initial` value corresponds to slang words generally, mispronounced words because of an accent such as _lyin'_ instead of _lying_.  \n",
    "The `alternative` is another way of displaying the subtitle for example HOW instead of how.\n",
    "\n",
    "There is another attribute we found for the strings and words which is not present in all the files and it is the `emphasis` attribute, which takes either true or false value.\n",
    "\n",
    "#### Metadata\n",
    "\n",
    "The `metadata` section has the following structure:\n",
    "\n",
    "```\n",
    "├── Conversion\n",
    "│   ├── corrected_words: Integer\n",
    "│   ├── sentences: Integer\n",
    "│   ├── tokens: Integer\n",
    "│   ├── encoding: String (always utf-8)\n",
    "│   ├── unknown_words: Integer\n",
    "│   ├── ignored_blocks: Integer\n",
    "│   ├── truecased_words: Integer\n",
    "├── Subtitle\n",
    "│   ├── language: String\n",
    "│   ├── date: String\n",
    "│   ├── duration: String\n",
    "│   ├── cds: String (presented as #/# where # is an int)\n",
    "│   ├── blocks: Integer\n",
    "│   ├── confidence: Double\n",
    "├── Source\n",
    "│   ├── genre: String[] (up to 3 genres)\n",
    "│   ├── year: Integer\n",
    "│   ├── duration: Integer (in minutes)\n",
    "│   ├── original: String\n",
    "│   ├── country: String\n",
    "```\n",
    "\n",
    "This is the structure of the metadata we consider, although some XML files may not have all the entries. \n",
    "We use the metadata to obtain additional information about the movie or show's subtitles and compute certain statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration\n",
    "\n",
    "After going through the dataset we found many things worth noting.  \n",
    "First of all is that the dataset is not uniform, it has \"strange folders\" and contains xml files that are not related to movies or tv shows. We have for example the folder 666/ which contains Justin Bieber song subtitles, folder 1858/ which is empty and so on.  \n",
    "To solve this we decided to ignore all the folders which weren't inside the range of 1920-2018. We also found that trailer of films are present in the dataset. In the folder 2018 we found for example Black Panther teaser trailer subtitles.\n",
    "\n",
    "Another thing worth mentioning is that a lot of different subtitles contain text that is not related to the movie, like credentials of the person who made the subtitles.\n",
    "\n",
    "We found that the code for the movies is not always reliable to get the actual movie name, hence we can't have 100% certainty that the id for the subtitles (identifier of the movie folder containing the substitles) are associated with the correct film.  \n",
    "We also see that each movie might have more than 1 subtitle file, we have to decide which one we should take. We can base this decision by taking one subtitle file at random or we could consider the confidence attribute in the metadata. \n",
    "To choose movies that can actually have a correct IMDb identifier we looked that the ID is composed of 7 integers, hence all the files in folders with more or less that 7 integers (after the year identifier) are very hard to associate with a video.  \n",
    "For the moment, as we only took a sample of the dataset from the cluster (see the python script `extract_sample_2.py`), we collected 1 or 2 movies for each year in the range 1920-2018. We made sure that the movie-id was of 7 digits. Half of the concerned folders are empty though. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDb Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have at our disposal the IMDb ratings and basics dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO scrape data https://datasets.imdbws.com/\n",
    "ratings_fn = \"title.ratings.tsv.gz\"\n",
    "basics_fn = \"title.basics.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------+\n",
      "|   tconst|averageRating|numVotes|\n",
      "+---------+-------------+--------+\n",
      "|tt0000001|          5.8|    1440|\n",
      "|tt0000002|          6.3|     172|\n",
      "|tt0000003|          6.6|    1041|\n",
      "|tt0000004|          6.4|     102|\n",
      "|tt0000005|          6.2|    1735|\n",
      "|tt0000006|          5.5|      91|\n",
      "|tt0000007|          5.5|     579|\n",
      "|tt0000008|          5.6|    1539|\n",
      "|tt0000009|          5.6|      74|\n",
      "|tt0000010|          6.9|    5127|\n",
      "|tt0000011|          5.4|     214|\n",
      "|tt0000012|          7.4|    8599|\n",
      "|tt0000013|          5.7|    1318|\n",
      "|tt0000014|          7.2|    3739|\n",
      "|tt0000015|          6.2|     660|\n",
      "|tt0000016|          5.9|     982|\n",
      "|tt0000017|          4.8|     197|\n",
      "|tt0000018|          5.5|     414|\n",
      "|tt0000019|          6.6|      13|\n",
      "|tt0000020|          5.1|     232|\n",
      "+---------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ratings = spark.read.option(\"header\", \"true\")\\\n",
    "                       .option(\"sep\", \"\\t\")\\\n",
    "                       .csv(\"imdb_data/\" + ratings_fn)\n",
    "df_ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|              genres|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|tt0000001|    short|          Carmencita|          Carmencita|      0|     1894|     \\N|             1|   Documentary,Short|\n",
      "|tt0000002|    short|Le clown et ses c...|Le clown et ses c...|      0|     1892|     \\N|             5|     Animation,Short|\n",
      "|tt0000003|    short|      Pauvre Pierrot|      Pauvre Pierrot|      0|     1892|     \\N|             4|Animation,Comedy,...|\n",
      "|tt0000004|    short|         Un bon bock|         Un bon bock|      0|     1892|     \\N|            \\N|     Animation,Short|\n",
      "|tt0000005|    short|    Blacksmith Scene|    Blacksmith Scene|      0|     1893|     \\N|             1|        Comedy,Short|\n",
      "|tt0000006|    short|   Chinese Opium Den|   Chinese Opium Den|      0|     1894|     \\N|             1|               Short|\n",
      "|tt0000007|    short|Corbett and Court...|Corbett and Court...|      0|     1894|     \\N|             1|         Short,Sport|\n",
      "|tt0000008|    short|Edison Kinetoscop...|Edison Kinetoscop...|      0|     1894|     \\N|             1|   Documentary,Short|\n",
      "|tt0000009|    movie|          Miss Jerry|          Miss Jerry|      0|     1894|     \\N|            45|             Romance|\n",
      "|tt0000010|    short|Employees Leaving...|La sortie de l'us...|      0|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000011|    short|Akrobatisches Pot...|Akrobatisches Pot...|      0|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000012|    short|The Arrival of a ...|L'arrivée d'un tr...|      0|     1896|     \\N|             1|   Documentary,Short|\n",
      "|tt0000013|    short|The Photographica...|Neuville-sur-Saôn...|      0|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000014|    short|Tables Turned on ...|   L'arroseur arrosé|      0|     1895|     \\N|             1|        Comedy,Short|\n",
      "|tt0000015|    short| Autour d'une cabine| Autour d'une cabine|      0|     1894|     \\N|             2|     Animation,Short|\n",
      "|tt0000016|    short|Barque sortant du...|Barque sortant du...|      0|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000017|    short|Italienischer Bau...|Italienischer Bau...|      0|     1895|     \\N|             1|   Documentary,Short|\n",
      "|tt0000018|    short|Das boxende Känguruh|Das boxende Känguruh|      0|     1895|     \\N|             1|               Short|\n",
      "|tt0000019|    short|    The Clown Barber|    The Clown Barber|      0|     1898|     \\N|            \\N|        Comedy,Short|\n",
      "|tt0000020|    short|      The Derby 1895|      The Derby 1895|      0|     1895|     \\N|             1|Documentary,Short...|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_basics = spark.read.option(\"header\", \"true\")\\\n",
    "                      .option(\"sep\", \"\\t\")\\\n",
    "                      .csv(\"imdb_data/\" + basics_fn)\n",
    "df_basics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample film loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take one sample film and load it into a spark dataframe with the help of spark-xml library. We know that we are dealing with a very big data set hence using spark is the right way to go. Using this library we see that we can load two distinct dataframes per movie which reveal different information. One that contains the actual text and another one that contains the metadata of the film."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have first the schema and look of the dataframe containing the subtitles. We can see that it is not very clear and it contains a lot of null values and information we want to get rid of. Each word array contains an Id we don't really need and per row entry we have an array of arrays for words and for the times. We need to decide how we want to store the information and what information we want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: long (nullable = true)\n",
      " |-- time: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _id: string (nullable = true)\n",
      " |    |    |-- _value: string (nullable = true)\n",
      " |-- w: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _id: double (nullable = true)\n",
      "\n",
      "+---+--------------------+--------------------+\n",
      "|_id|                time|                   w|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|[[, T1S, 00:00:10...|[[\", 1.1], [ahmad...|\n",
      "|  2|[[, T2S, 00:00:56...|[[Well, 2.1], [,,...|\n",
      "|  3|[[, T3S, 00:00:58...|[[What, 3.1], [ki...|\n",
      "|  4|[[, T5S, 00:01:09...|[[Crazy, 4.1], [....|\n",
      "|  5|[[, T6S, 00:01:10...|[[Got, 5.1], [me,...|\n",
      "|  6|[[, T7S, 00:01:18...|[[Serious, 6.1], ...|\n",
      "|  7|[[, T7E, 00:01:23...|[[I, 7.1], [feel,...|\n",
      "|  8|[[, T8S, 00:01:25...|[[Alright, 8.1], ...|\n",
      "|  9|[[, T8E, 00:01:29...|[[See, 9.1], [ya,...|\n",
      "| 10|[[, T9S, 00:01:32...|[[Okay, 10.1], [,...|\n",
      "| 11|[[, T10S, 00:01:3...|[[It, 11.1], ['s,...|\n",
      "| 12|[[, T11S, 00:01:4...|[[Okay, 12.1], [,...|\n",
      "| 13|[[, T12S, 00:01:4...|[[Keep, 13.1], [g...|\n",
      "| 14|[[, T13S, 00:01:4...|[[I, 14.1], [thin...|\n",
      "| 15|[[, T14S, 00:01:5...|[[Go, 15.1], [str...|\n",
      "| 16|[[, T15S, 00:02:1...|[[I, 16.1], [just...|\n",
      "| 17|                null|[[I, 17.1], [don,...|\n",
      "| 18|[[, T15E, 00:02:1...|[[Just, 18.1], [k...|\n",
      "| 19|[[, T16S, 00:02:2...|[[Fuck, 19.1], [t...|\n",
      "| 20|[[, T17S, 00:02:2...|[[Not, 20.1], [to...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample_film = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "                                .options(rowTag='s') \\\n",
    "                                .load('data_subtitles/2017/5052448/6963336.xml.gz')\n",
    "df_sample_film.printSchema()\n",
    "df_sample_film.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_metadata = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "                                    .options(rowTag='meta') \\\n",
    "                                    .load('data_subtitles/2017/5052448/6963336.xml.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to treat the dataframes now to store the information we actually we want in an efficient manner. Here we use our sample film to create functions that will shape our dataframes to then be able to extract the information we desire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the metadata we have a very clean dataframe which can be used for a lot of statistics and filtering. We have useful stats such as the duration of the film, the genre. Here we can see the schema. We need to decide what is actually relevant for us to filter out the useless information and choose which format our dataframe should have (for example having all the different genres in a separate column.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- conversion: struct (nullable = true)\n",
      " |    |-- corrected_words: long (nullable = true)\n",
      " |    |-- encoding: string (nullable = true)\n",
      " |    |-- ignored_blocks: long (nullable = true)\n",
      " |    |-- sentences: long (nullable = true)\n",
      " |    |-- tokens: long (nullable = true)\n",
      " |    |-- truecased_words: long (nullable = true)\n",
      " |    |-- unknown_words: long (nullable = true)\n",
      " |-- source: struct (nullable = true)\n",
      " |    |-- duration: long (nullable = true)\n",
      " |    |-- genre: string (nullable = true)\n",
      " |    |-- year: long (nullable = true)\n",
      " |-- subtitle: struct (nullable = true)\n",
      " |    |-- blocks: long (nullable = true)\n",
      " |    |-- cds: string (nullable = true)\n",
      " |    |-- confidence: double (nullable = true)\n",
      " |    |-- date: string (nullable = true)\n",
      " |    |-- duration: string (nullable = true)\n",
      " |    |-- language: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|          conversion|              source|            subtitle|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[0, utf-8, 0, 129...|[104, Horror,Myst...|[858, 1/1, 1.0, 2...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample_metadata.printSchema()\n",
    "df_sample_metadata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is no actual link between our both dataframes. The id of the film is only present in the folder which contains the different subtitle files. We need to be able to link the subtitle and metadata dataframe. To do so we add an id column which contains the id of the film."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a function which will give us a resulting metadata dataframe with the information we want. We separate genres as an array of strings for later queries. We associate the imdb Id aswell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO check if convert genres to lowercase?\n",
    "#TODO what to do if missing columns\n",
    "imdb_id = \"5052448\"\n",
    "udf_split = udf(str.split, ArrayType(StringType()))\n",
    "def meta_data_filter(df_metadata, identifier):\n",
    "    df = df_metadata.withColumn(\"imdb_id\", lit(identifier)).selectExpr(\"imdb_id\", \"conversion.sentences\",\\\n",
    "                                                                   \"source.duration\", \"source.genre\", \\\n",
    "                                                                   \"source.year\", \"subtitle.blocks\", \\\n",
    "                                                                   \"subtitle.duration as subtitle_duration\",\"subtitle.language\")\n",
    "    df = df.withColumn(\"genres\", udf_split(\"genre\")).drop(\"genre\")\n",
    "    return df\n",
    "df_md = meta_data_filter(df_sample_metadata, imdb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+----+------+-----------------+--------+----------------+\n",
      "|imdb_id|sentences|duration|year|blocks|subtitle_duration|language|          genres|\n",
      "+-------+---------+--------+----+------+-----------------+--------+----------------+\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|\n",
      "+-------+---------+--------+----+------+-----------------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_md.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a proper representation for our metadata, now we want to do the same for the subtitles. We consider storing the sentences in a list of words as it seems to be the best way to answer queries such as the number of distinct words, words in common between films, etc. We create anyway a second function which gives us the sentence as a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentence(words):\n",
    "    w_list = []\n",
    "    for w in words:\n",
    "        if w['_VALUE'] != '':\n",
    "            w_list.append(w['_VALUE'])\n",
    "    return w_list\n",
    "udf_word_array = udf(to_sentence, ArrayType(StringType()))\n",
    "udf_sentence = udf(lambda x: ' '.join([w[0] for w in x]), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "|_id|                time|                   w|            sentence|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  1|[[, T1S, 00:00:10...|[[\", 1.1], [ahmad...|[\", ahmad, torifi...|\n",
      "|  2|[[, T2S, 00:00:56...|[[Well, 2.1], [,,...|[Well, ,, the, th...|\n",
      "|  3|[[, T3S, 00:00:58...|[[What, 3.1], [ki...|[What, kinda, sic...|\n",
      "|  4|[[, T5S, 00:01:09...|[[Crazy, 4.1], [....|          [Crazy, .]|\n",
      "|  5|[[, T6S, 00:01:10...|[[Got, 5.1], [me,...|[Got, me, out, in...|\n",
      "|  6|[[, T7S, 00:01:18...|[[Serious, 6.1], ...|[Serious, though, .]|\n",
      "|  7|[[, T7E, 00:01:23...|[[I, 7.1], [feel,...|[I, feel, here, l...|\n",
      "|  8|[[, T8S, 00:01:25...|[[Alright, 8.1], ...|[Alright, man, ,,...|\n",
      "|  9|[[, T8E, 00:01:29...|[[See, 9.1], [ya,...|        [See, ya, .]|\n",
      "| 10|[[, T9S, 00:01:32...|[[Okay, 10.1], [,...|[Okay, ,, so, thi...|\n",
      "| 11|[[, T10S, 00:01:3...|[[It, 11.1], ['s,...|[It, 's, like, a,...|\n",
      "| 12|[[, T11S, 00:01:4...|[[Okay, 12.1], [,...|[Okay, ,, let, 's...|\n",
      "| 13|[[, T12S, 00:01:4...|[[Keep, 13.1], [g...|[Keep, going, str...|\n",
      "| 14|[[, T13S, 00:01:4...|[[I, 14.1], [thin...|[I, think, he, sa...|\n",
      "| 15|[[, T14S, 00:01:5...|[[Go, 15.1], [str...|[Go, straight, ,,...|\n",
      "| 16|[[, T15S, 00:02:1...|[[I, 16.1], [just...|[I, just, keep, o...|\n",
      "| 17|                null|[[I, 17.1], [don,...|[I, don, 't, do, ...|\n",
      "| 18|[[, T15E, 00:02:1...|[[Just, 18.1], [k...|[Just, keep, on, ...|\n",
      "| 19|[[, T16S, 00:02:2...|[[Fuck, 19.1], [t...|[Fuck, this, ,, I...|\n",
      "| 20|[[, T17S, 00:02:2...|[[Not, 20.1], [to...|     [Not, today, .]|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample_film_sentence_list = df_sample_film.withColumn(\"sentence\", udf_word_array(\"w\"))\n",
    "df_sample_film_sentence_list.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing the subtitle dataframe, we encounterd the problem of not being able to associate words with timestamps. As our xml files separate data by sentences, each sentence might have 0 or many timestamps associated and it would be necessary to change the whole dataset to be able to associate a word with a given timestamp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "|_id|                time|                   w|            sentence|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  1|[[, T1S, 00:00:10...|[[\", 1.1], [ahmad...|\" ahmad torifi \" ...|\n",
      "|  2|[[, T2S, 00:00:56...|[[Well, 2.1], [,,...|Well , the thing ...|\n",
      "|  3|[[, T3S, 00:00:58...|[[What, 3.1], [ki...|What kinda sick i...|\n",
      "|  4|[[, T5S, 00:01:09...|[[Crazy, 4.1], [....|             Crazy .|\n",
      "|  5|[[, T6S, 00:01:10...|[[Got, 5.1], [me,...|Got me out in thi...|\n",
      "|  6|[[, T7S, 00:01:18...|[[Serious, 6.1], ...|    Serious though .|\n",
      "|  7|[[, T7E, 00:01:23...|[[I, 7.1], [feel,...|I feel here like ...|\n",
      "|  8|[[, T8S, 00:01:25...|[[Alright, 8.1], ...|Alright man , alr...|\n",
      "|  9|[[, T8E, 00:01:29...|[[See, 9.1], [ya,...|            See ya .|\n",
      "| 10|[[, T9S, 00:01:32...|[[Okay, 10.1], [,...|Okay , so this is...|\n",
      "| 11|[[, T10S, 00:01:3...|[[It, 11.1], ['s,...|It 's like a fuck...|\n",
      "| 12|[[, T11S, 00:01:4...|[[Okay, 12.1], [,...|Okay , let 's see...|\n",
      "| 13|[[, T12S, 00:01:4...|[[Keep, 13.1], [g...|Keep going straig...|\n",
      "| 14|[[, T13S, 00:01:4...|[[I, 14.1], [thin...|I think he said t...|\n",
      "| 15|[[, T14S, 00:01:5...|[[Go, 15.1], [str...|Go straight , the...|\n",
      "| 16|[[, T15S, 00:02:1...|[[I, 16.1], [just...|I just keep on wa...|\n",
      "| 17|                null|[[I, 17.1], [don,...|I don 't do nothi...|\n",
      "| 18|[[, T15E, 00:02:1...|[[Just, 18.1], [k...|    Just keep on ...|\n",
      "| 19|[[, T16S, 00:02:2...|[[Fuck, 19.1], [t...|Fuck this , I 'm ...|\n",
      "| 20|[[, T17S, 00:02:2...|[[Not, 20.1], [to...|         Not today .|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample_film_sentence_string = df_sample_film.withColumn(\"sentence\", udf_sentence(\"w\"))\n",
    "df_sample_film_sentence_string.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_film_words =df_sample_film_sentence_list.select('*', explode(col(\"sentence\")).alias('word'))\n",
    "#filter strings that are not words like marks or spaces, we use a regular expression.\n",
    "df_sample_film_words =df_sample_film_words.filter(df_sample_film_words.word.rlike(\"^[a-zA-Z]+$\"))\n",
    "word_count_distinct = df_sample_film_words.select(\"word\").distinct().count()\n",
    "word_count_total = df_sample_film_words.select(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct words in film is: 1452\n",
      "Total number of  words in film is: 6798\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of distinct words in film is: {:}\".format(word_count_distinct))\n",
    "print(\"Total number of  words in film is: {:}\".format(word_count_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def film_words(df_film):\n",
    "    df_words = df_film.withColumn(\"sentence\", udf_word_array(\"w\")) \\\n",
    "                        .select('*', explode(col(\"sentence\")).alias('word'))\n",
    "    #TODO change udf_sentence to filter out empty strings and marks.\n",
    "    df_words_filter = df_words.filter(df_words.word.rlike(\"^[a-zA-Z]+$\"))\n",
    "    word_count_distinct = df_words_filter.select(\"word\").distinct().count()\n",
    "    word_count_total = df_words_filter.select(\"word\").count()\n",
    "    return (df_words_filter, word_count_distinct, word_count_total)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|imdb_id|_id|      word|\n",
      "+-------+---+----------+\n",
      "|5052448|  1|     ahmad|\n",
      "|5052448|  1|    torifi|\n",
      "|5052448|  1|  subtitle|\n",
      "|5052448|  2|      Well|\n",
      "|5052448|  2|       the|\n",
      "|5052448|  2|     thing|\n",
      "|5052448|  2|         I|\n",
      "|5052448|  2|      been|\n",
      "|5052448|  2|    asking|\n",
      "|5052448|  2|    myself|\n",
      "|5052448|  2|        is|\n",
      "|5052448|  3|      What|\n",
      "|5052448|  3|     kinda|\n",
      "|5052448|  3|      sick|\n",
      "|5052448|  3|individual|\n",
      "|5052448|  3|     names|\n",
      "|5052448|  3|         a|\n",
      "|5052448|  3|    street|\n",
      "|5052448|  3|  Edgewood|\n",
      "|5052448|  3|       Way|\n",
      "+-------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def subtitle_df(df_film, identifier):\n",
    "    (df_words, word_count, total_words) = film_words(df_film)\n",
    "    df_result = df_words.withColumn(\"imdb_id\", lit(identifier)).select(\"imdb_id\", \"_id\", \"word\")\n",
    "    return (df_result, word_count, word_count_total)\n",
    "film, words_distinct, words_total = subtitle_df(df_sample_film, imdb_id)\n",
    "film.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+----+------+-----------------+--------+----------------+-------+---+----------+\n",
      "|imdb_id|sentences|duration|year|blocks|subtitle_duration|language|          genres|imdb_id|_id|      word|\n",
      "+-------+---------+--------+----+------+-----------------+--------+----------------+-------+---+----------+\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  1|     ahmad|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  1|    torifi|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  1|  subtitle|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  2|      Well|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  2|       the|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  2|     thing|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  2|         I|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  2|      been|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  2|    asking|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  2|    myself|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  2|        is|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  3|      What|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  3|     kinda|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  3|      sick|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  3|individual|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  3|     names|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  3|         a|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  3|    street|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  3|  Edgewood|\n",
      "|5052448|     1293|     104|2017|   858|     01:40:21,909| English|[Horror,Mystery]|5052448|  3|       Way|\n",
      "+-------+---------+--------+----+------+-----------------+--------+----------------+-------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "df_md.join(film, film[\"imdb_id\"] == df_md[\"imdb_id\"], how=\"cross\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1452"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
