{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Movie behind a Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import urllib.request\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.10:0.4.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set('spark.sql.session.timeZone', 'UTC')\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of datasets\n",
    "\n",
    "The OpenSubtitles dataset is a compressed cluster of folders containing XML files. Each XML file is split into a script portion with the subtitles of the movie and a metadata portion with additional information about the movie or show. The name of one of the parent folders of the XML file is the corresponding IMDb identifier of the movie or show, thus allowing us to extract additional information from the IMDb dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenSubtitles\n",
    "\n",
    "The dataset consists of 31 (**TODO: how many?**) GB of XML files distributed in the following file structure: \n",
    "\n",
    "```\n",
    "├── opensubtitle\n",
    "│   ├── OpenSubtitles2018\n",
    "│   │   ├── Year\n",
    "│   │   │   ├── Id\n",
    "│   │   │   │   ├── #######.xml.gz\n",
    "│   │   │   │   ├── #######.xml.gz\n",
    "│   ├── en.tar.gz\n",
    "│   ├── fr.tar.gz\n",
    "│   ├── zh_cn.tar.gz\n",
    "```\n",
    "where\n",
    "- `######` is a 6-digit unique identifier of the file on the OpenSubtitles dataset.\n",
    "- `Year` is the year the movie or episode was made.\n",
    "- `Id` is a 5 to 7 digit identifier (if it's 7-digit it's also an IMDb identifier).\n",
    "\n",
    "The subtitles are provided in different languages. For the moment we only analyze the `OpenSubtitles2018` folder and it's the only folder we detail.\n",
    "\n",
    "Some `Year` folders are not indicative, for instance 0, 666 and 1191. We also notice that for each `Id` we can find multiple subtitle XML files, as illustrated above. The decompressed XML files vary in size, ranging from 5KB to 9000KB sized files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML Files\n",
    "\n",
    "#### Subtitles\n",
    "\n",
    "For the actual subtitles in each xml file we can see that they are stored in sentences, each one having an unique id(integers in increasing order starting at 1). Each sentence has a set timestamps and a set of words. Every timestamp and word have also an id and a set of attributes. The timestamp id has two different formats. \n",
    "\n",
    "First for the timestamp id we have \"T#S\" or \"T#E\" where # is an increasing integer, \"S\" indicates start and \"E\" indicates end. The words inbetween a start and end of timestamp are shown on the screen during the time indicated by the time stamp. **This is a great indicator of fast dialog!**. Apart from the id, the timestamp also has a value attribute which has the format `` HH:mm:ss,fff``.\n",
    "\n",
    "For the words the id is simply an increasing number of decimal numbers \"X.Y\" where X is the string id and Y is the word id within the corresponding string. Each word element in the XML file has a non-empty value (the actual word, can be a mark) and it might have an alternative and initial value. The initial value corresponds to slang words generally, mispronounced words because of an accent such as lyin' instead of lying. The alternative is another way of displaying the subtitle for example HOW instead of how.\n",
    "\n",
    "There is another attribute we found for the strings and words which is not present in all the files and it is the emphasis attribute, which takes either true or false value.\n",
    "\n",
    "#### Metadata\n",
    "\n",
    "Each XML file has a unique identifier in the name of the file and contains at the end of the file metadata in the following structure:\n",
    "\n",
    "```\n",
    "├── Conversion\n",
    "│   ├── corrected_words: Integer\n",
    "│   ├── sentences: Integer\n",
    "│   ├── tokens: Integer\n",
    "│   ├── encoding: String always utf-8\n",
    "│   ├── unknown_words: Integer\n",
    "│   ├── ignored_blocks: Integer\n",
    "│   ├── truecased_words: Integer\n",
    "├── Subtitle\n",
    "│   ├── language: String\n",
    "│   ├── date: String\n",
    "│   ├── duration: String\n",
    "│   ├── cds: String presented as #/# where # is an int\n",
    "│   ├── blocks: Integer\n",
    "│   ├── confidence: Double\n",
    "├── Source\n",
    "│   ├── genre: String[] (up to 3 genres)\n",
    "│   ├── year: Integer\n",
    "│   ├── duration: Integer (in minutes)\n",
    "│   ├── original: String\n",
    "│   ├── country: String\n",
    "```\n",
    "\n",
    "This is the structure of the metadata we consider, although some XML files may not have all the entries. \n",
    "We use the metadata to obtain additional information about the movie or show's subtitles and compute certain statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration\n",
    "\n",
    "After going through the dataset we found many things worth noting. First of all is that the dataset is not uniform, it has \"strange folders\" and contains xml files that are not related to movies or tv shows. We have for example the folder 666/ which contains Justin Bieber song subtitles, folder 1858/ which is empty and so on. To solve this we decided to ignore all the folders which weren't inside the range of 1920-2018. We also found that trailer of films are present in the dataset. In the folder 2018 we found for example Black Panther teaser trailer subtitles.\n",
    "\n",
    "Another thing worth mentioning is that a lot of different subtitles contain text that is not related to the movie, like credentials of the person who made the subtitles.\n",
    "\n",
    "We found that the code for the movies is not always reliable to get the actual movie name, hence we can't have 100% certainty that the id for the subtitles are associated with the correct film. We also see that each movie might have more than 1 subtitle file, we have to decide which one we should take. We can base this decision by taking one subtitle file at random or we could consider the confidence attribute in the metadata. To choose movies that can actually have a correct IMDb identifier we looked that the ID is composed of 7 integers, hence all the files in folders with more or less that 7 integers (after the year identifier) are very hard to associate with a video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDb Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have at our disposal the IMDb ratings and basics dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseURL = \"https://datasets.imdbws.com/\"\n",
    "ratings_fn = \"title.ratings.tsv.gz\"\n",
    "basics_fn = \"title.basics.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>averageRating</th>\n",
       "      <th>numVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0000001</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0000002</td>\n",
       "      <td>6.3</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0000003</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0000004</td>\n",
       "      <td>6.4</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0000005</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tconst  averageRating  numVotes\n",
       "0  tt0000001            5.8      1439\n",
       "1  tt0000002            6.3       172\n",
       "2  tt0000003            6.6      1040\n",
       "3  tt0000004            6.4       102\n",
       "4  tt0000005            6.2      1735"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings = pd.read_csv(baseURL + ratings_fn, sep='\\t', compression='gzip')\n",
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>titleType</th>\n",
       "      <th>primaryTitle</th>\n",
       "      <th>originalTitle</th>\n",
       "      <th>isAdult</th>\n",
       "      <th>startYear</th>\n",
       "      <th>endYear</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0000001</td>\n",
       "      <td>short</td>\n",
       "      <td>Carmencita</td>\n",
       "      <td>Carmencita</td>\n",
       "      <td>0</td>\n",
       "      <td>1894</td>\n",
       "      <td>\\N</td>\n",
       "      <td>1</td>\n",
       "      <td>Documentary,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0000002</td>\n",
       "      <td>short</td>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "      <td>0</td>\n",
       "      <td>1892</td>\n",
       "      <td>\\N</td>\n",
       "      <td>5</td>\n",
       "      <td>Animation,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0000003</td>\n",
       "      <td>short</td>\n",
       "      <td>Pauvre Pierrot</td>\n",
       "      <td>Pauvre Pierrot</td>\n",
       "      <td>0</td>\n",
       "      <td>1892</td>\n",
       "      <td>\\N</td>\n",
       "      <td>4</td>\n",
       "      <td>Animation,Comedy,Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0000004</td>\n",
       "      <td>short</td>\n",
       "      <td>Un bon bock</td>\n",
       "      <td>Un bon bock</td>\n",
       "      <td>0</td>\n",
       "      <td>1892</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>Animation,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0000005</td>\n",
       "      <td>short</td>\n",
       "      <td>Blacksmith Scene</td>\n",
       "      <td>Blacksmith Scene</td>\n",
       "      <td>0</td>\n",
       "      <td>1893</td>\n",
       "      <td>\\N</td>\n",
       "      <td>1</td>\n",
       "      <td>Comedy,Short</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tconst titleType            primaryTitle           originalTitle  \\\n",
       "0  tt0000001     short              Carmencita              Carmencita   \n",
       "1  tt0000002     short  Le clown et ses chiens  Le clown et ses chiens   \n",
       "2  tt0000003     short          Pauvre Pierrot          Pauvre Pierrot   \n",
       "3  tt0000004     short             Un bon bock             Un bon bock   \n",
       "4  tt0000005     short        Blacksmith Scene        Blacksmith Scene   \n",
       "\n",
       "   isAdult startYear endYear runtimeMinutes                    genres  \n",
       "0        0      1894      \\N              1         Documentary,Short  \n",
       "1        0      1892      \\N              5           Animation,Short  \n",
       "2        0      1892      \\N              4  Animation,Comedy,Romance  \n",
       "3        0      1892      \\N             \\N           Animation,Short  \n",
       "4        0      1893      \\N              1              Comedy,Short  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_basics = pd.read_csv(baseURL + basics_fn, sep='\\t', compression='gzip')\n",
    "df_basics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample film loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take one sample film and load it into a spark dataframe with the help of spark-xml library. We know that we are dealing with a very big data set hence using spark is the right way to go. Using this library we see that we can load two distinct dataframes per movie which reveal different information. One that contains the actual text and another one that contains the metadata of the film."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have first the schema and look of the dataframe containing the subtitles. We can see that it is not very clear and it contains a lot of null values and information we want to get rid of. Each word array contains an Id we don't really need and per row entry we have an array of arrays for words and for the times. We need to decide how we want to store the information and what information we want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: long (nullable = true)\n",
      " |-- time: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _id: string (nullable = true)\n",
      " |    |    |-- _value: string (nullable = true)\n",
      " |-- w: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _id: double (nullable = true)\n",
      "\n",
      "+---+--------------------+--------------------+\n",
      "|_id|                time|                   w|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|[[, T1S, 00:00:10...|[[\", 1.1], [ahmad...|\n",
      "|  2|[[, T2S, 00:00:56...|[[Well, 2.1], [,,...|\n",
      "|  3|[[, T3S, 00:00:58...|[[What, 3.1], [ki...|\n",
      "|  4|[[, T5S, 00:01:09...|[[Crazy, 4.1], [....|\n",
      "|  5|[[, T6S, 00:01:10...|[[Got, 5.1], [me,...|\n",
      "|  6|[[, T7S, 00:01:18...|[[Serious, 6.1], ...|\n",
      "|  7|[[, T7E, 00:01:23...|[[I, 7.1], [feel,...|\n",
      "|  8|[[, T8S, 00:01:25...|[[Alright, 8.1], ...|\n",
      "|  9|[[, T8E, 00:01:29...|[[See, 9.1], [ya,...|\n",
      "| 10|[[, T9S, 00:01:32...|[[Okay, 10.1], [,...|\n",
      "| 11|[[, T10S, 00:01:3...|[[It, 11.1], ['s,...|\n",
      "| 12|[[, T11S, 00:01:4...|[[Okay, 12.1], [,...|\n",
      "| 13|[[, T12S, 00:01:4...|[[Keep, 13.1], [g...|\n",
      "| 14|[[, T13S, 00:01:4...|[[I, 14.1], [thin...|\n",
      "| 15|[[, T14S, 00:01:5...|[[Go, 15.1], [str...|\n",
      "| 16|[[, T15S, 00:02:1...|[[I, 16.1], [just...|\n",
      "| 17|                null|[[I, 17.1], [don,...|\n",
      "| 18|[[, T15E, 00:02:1...|[[Just, 18.1], [k...|\n",
      "| 19|[[, T16S, 00:02:2...|[[Fuck, 19.1], [t...|\n",
      "| 20|[[, T17S, 00:02:2...|[[Not, 20.1], [to...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample_film = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "                                .options(rowTag='s') \\\n",
    "                                .load('data_subtitles/2017/5052448/6963336.xml.gz')\n",
    "df_sample_film.printSchema()\n",
    "df_sample_film.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_metadata = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "                                    .options(rowTag='meta') \\\n",
    "                                    .load('data_subtitles/2017/5052448/6963336.xml.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the metadata we have a very clean dataframe which can be used for a lot of statistics and filtering. We have useful stats such as the duration of the film, the genre. Here we can see the schema. We need to decide what is actually relevant for us to filter out the useless information and choose which format our dataframe should have (for example having all the different genres in a separate column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- conversion: struct (nullable = true)\n",
      " |    |-- corrected_words: long (nullable = true)\n",
      " |    |-- encoding: string (nullable = true)\n",
      " |    |-- ignored_blocks: long (nullable = true)\n",
      " |    |-- sentences: long (nullable = true)\n",
      " |    |-- tokens: long (nullable = true)\n",
      " |    |-- truecased_words: long (nullable = true)\n",
      " |    |-- unknown_words: long (nullable = true)\n",
      " |-- source: struct (nullable = true)\n",
      " |    |-- duration: long (nullable = true)\n",
      " |    |-- genre: string (nullable = true)\n",
      " |    |-- year: long (nullable = true)\n",
      " |-- subtitle: struct (nullable = true)\n",
      " |    |-- blocks: long (nullable = true)\n",
      " |    |-- cds: string (nullable = true)\n",
      " |    |-- confidence: double (nullable = true)\n",
      " |    |-- date: string (nullable = true)\n",
      " |    |-- duration: string (nullable = true)\n",
      " |    |-- language: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|          conversion|              source|            subtitle|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[0, utf-8, 0, 129...|[104, Horror,Myst...|[858, 1/1, 1.0, 2...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample_metadata.printSchema()\n",
    "df_sample_metadata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is no actual link between our both dataframes. The id of the film is only present in the folder which contains the different subtitle files. We need to be able to link the subtitle and metadata dataframe. To do so we add an id column which contains the id of the film."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to treat the dataframes now to store the information we actually we want in an efficient manner. Here we use our sample film to create functions that will shape our dataframes to then be able to extract the information we desire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentence(words):\n",
    "    w_list = []\n",
    "    for w in words:\n",
    "        w_list.append(w[0])\n",
    "    return w_list\n",
    "udf_word = udf(to_sentence, ArrayType(StringType()))\n",
    "udf_sentence = udf(lambda x: ' '.join([w[0] for w in x]), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "|_id|                time|                   w|            sentence|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  1|[[, T1S, 00:00:10...|[[\", 1.1], [ahmad...|[\", ahmad, torifi...|\n",
      "|  2|[[, T2S, 00:00:56...|[[Well, 2.1], [,,...|[Well, ,, the, th...|\n",
      "|  3|[[, T3S, 00:00:58...|[[What, 3.1], [ki...|[What, kinda, sic...|\n",
      "|  4|[[, T5S, 00:01:09...|[[Crazy, 4.1], [....|          [Crazy, .]|\n",
      "|  5|[[, T6S, 00:01:10...|[[Got, 5.1], [me,...|[Got, me, out, in...|\n",
      "|  6|[[, T7S, 00:01:18...|[[Serious, 6.1], ...|[Serious, though, .]|\n",
      "|  7|[[, T7E, 00:01:23...|[[I, 7.1], [feel,...|[I, feel, here, l...|\n",
      "|  8|[[, T8S, 00:01:25...|[[Alright, 8.1], ...|[Alright, man, ,,...|\n",
      "|  9|[[, T8E, 00:01:29...|[[See, 9.1], [ya,...|        [See, ya, .]|\n",
      "| 10|[[, T9S, 00:01:32...|[[Okay, 10.1], [,...|[Okay, ,, so, thi...|\n",
      "| 11|[[, T10S, 00:01:3...|[[It, 11.1], ['s,...|[It, 's, like, a,...|\n",
      "| 12|[[, T11S, 00:01:4...|[[Okay, 12.1], [,...|[Okay, ,, let, 's...|\n",
      "| 13|[[, T12S, 00:01:4...|[[Keep, 13.1], [g...|[Keep, going, str...|\n",
      "| 14|[[, T13S, 00:01:4...|[[I, 14.1], [thin...|[I, think, he, sa...|\n",
      "| 15|[[, T14S, 00:01:5...|[[Go, 15.1], [str...|[Go, straight, ,,...|\n",
      "| 16|[[, T15S, 00:02:1...|[[I, 16.1], [just...|[I, just, keep, o...|\n",
      "| 17|                null|[[I, 17.1], [don,...|[I, don, 't, do, ...|\n",
      "| 18|[[, T15E, 00:02:1...|[[Just, 18.1], [k...|[Just, keep, on, ...|\n",
      "| 19|[[, T16S, 00:02:2...|[[Fuck, 19.1], [t...|[Fuck, this, ,, I...|\n",
      "| 20|[[, T17S, 00:02:2...|[[Not, 20.1], [to...|     [Not, today, .]|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample_film_sentence_list = df_sample_film.withColumn(\"sentence\", udf_word(\"w\"))\n",
    "df_sample_film_sentence_list.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing the subtitle dataframe, we encounterd the problem of not being able to associate words with timestamps. As our xml files separate data by sentences, each sentence might have 0 or many timestamps associated and it would be necessary to change the whole dataset to b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "|_id|                time|                   w|            sentence|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  1|[[, T1S, 00:00:10...|[[\", 1.1], [ahmad...|\" ahmad torifi \" ...|\n",
      "|  2|[[, T2S, 00:00:56...|[[Well, 2.1], [,,...|Well , the thing ...|\n",
      "|  3|[[, T3S, 00:00:58...|[[What, 3.1], [ki...|What kinda sick i...|\n",
      "|  4|[[, T5S, 00:01:09...|[[Crazy, 4.1], [....|             Crazy .|\n",
      "|  5|[[, T6S, 00:01:10...|[[Got, 5.1], [me,...|Got me out in thi...|\n",
      "|  6|[[, T7S, 00:01:18...|[[Serious, 6.1], ...|    Serious though .|\n",
      "|  7|[[, T7E, 00:01:23...|[[I, 7.1], [feel,...|I feel here like ...|\n",
      "|  8|[[, T8S, 00:01:25...|[[Alright, 8.1], ...|Alright man , alr...|\n",
      "|  9|[[, T8E, 00:01:29...|[[See, 9.1], [ya,...|            See ya .|\n",
      "| 10|[[, T9S, 00:01:32...|[[Okay, 10.1], [,...|Okay , so this is...|\n",
      "| 11|[[, T10S, 00:01:3...|[[It, 11.1], ['s,...|It 's like a fuck...|\n",
      "| 12|[[, T11S, 00:01:4...|[[Okay, 12.1], [,...|Okay , let 's see...|\n",
      "| 13|[[, T12S, 00:01:4...|[[Keep, 13.1], [g...|Keep going straig...|\n",
      "| 14|[[, T13S, 00:01:4...|[[I, 14.1], [thin...|I think he said t...|\n",
      "| 15|[[, T14S, 00:01:5...|[[Go, 15.1], [str...|Go straight , the...|\n",
      "| 16|[[, T15S, 00:02:1...|[[I, 16.1], [just...|I just keep on wa...|\n",
      "| 17|                null|[[I, 17.1], [don,...|I don 't do nothi...|\n",
      "| 18|[[, T15E, 00:02:1...|[[Just, 18.1], [k...|    Just keep on ...|\n",
      "| 19|[[, T16S, 00:02:2...|[[Fuck, 19.1], [t...|Fuck this , I 'm ...|\n",
      "| 20|[[, T17S, 00:02:2...|[[Not, 20.1], [to...|         Not today .|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample_film_sentence_string = df_sample_film.withColumn(\"sentence\", udf_sentence(\"w\"))\n",
    "df_sample_film_sentence_string.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_film_words =df_sample_film_sentence_list.select('*', explode(col(\"sentence\")).alias('word'))\n",
    "#filter strings that are not words like marks or spaces, we use a regular expression.\n",
    "df_sample_film_words =df_sample_film_words.filter(df_sample_film_words.word.rlike(\"^[a-zA-Z]+$\"))\n",
    "word_count_distinct = df_sample_film_words.select(\"word\").distinct().count()\n",
    "word_count_total = df_sample_film_words.select(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct words in film is: 1452\n",
      "Total number of  words in film is: 6798\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of distinct words in film is: {:}\".format(word_count_distinct))\n",
    "print(\"Total number of  words in film is: {:}\".format(word_count_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def film_words(df_film):\n",
    "    df_words = df_film.withColumn(\"sentence\", udf_word(\"w\")) \\\n",
    "                        .select('*', explode(col(\"sentence\")).alias('word'))\n",
    "    #TODO change udf_sentence to filter out empty strings and marks.\n",
    "    df_words_filter = df_words.filter(df_sample_film_words.word.rlike(\"^[a-zA-Z]+$\"))\n",
    "    word_count_distinct = df_words_filter.select(\"word\").distinct().count()\n",
    "    word_count_total = df_words_filter.select(\"word\").count()\n",
    "    return (word_count_distinct, word_count_total)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
