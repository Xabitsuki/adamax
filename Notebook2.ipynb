{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Movie behind a Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "#from datetime import datetime\n",
    "#import urllib.request\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as psf\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.10:0.4.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set('spark.sql.session.timeZone', 'UTC')\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of datasets\n",
    "\n",
    "The OpenSubtitles dataset is a compressed cluster of folders containing XML files. Each XML file is split into a script portion with the subtitles of the movie and a metadata portion with additional information about the movie or show. The name of one of the parent folders of the XML file is the corresponding IMDb identifier of the movie or show, thus allowing us to extract additional information from the IMDb dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDb Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have at our disposal the IMDb ratings and basics dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO scrape data https://datasets.imdbws.com/\n",
    "ratings_fn = \"title.ratings.tsv.gz\"\n",
    "basics_fn = \"title.basics.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------+\n",
      "|   tconst|averageRating|numVotes|\n",
      "+---------+-------------+--------+\n",
      "|tt0000001|          5.8|    1440|\n",
      "|tt0000002|          6.3|     172|\n",
      "|tt0000003|          6.6|    1041|\n",
      "|tt0000004|          6.4|     102|\n",
      "|tt0000005|          6.2|    1735|\n",
      "|tt0000006|          5.5|      91|\n",
      "|tt0000007|          5.5|     579|\n",
      "|tt0000008|          5.6|    1539|\n",
      "|tt0000009|          5.6|      74|\n",
      "|tt0000010|          6.9|    5127|\n",
      "|tt0000011|          5.4|     214|\n",
      "|tt0000012|          7.4|    8599|\n",
      "|tt0000013|          5.7|    1318|\n",
      "|tt0000014|          7.2|    3739|\n",
      "|tt0000015|          6.2|     660|\n",
      "|tt0000016|          5.9|     982|\n",
      "|tt0000017|          4.8|     197|\n",
      "|tt0000018|          5.5|     414|\n",
      "|tt0000019|          6.6|      13|\n",
      "|tt0000020|          5.1|     232|\n",
      "+---------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ratings = spark.read.option(\"header\", \"true\")\\\n",
    "                       .option(\"sep\", \"\\t\")\\\n",
    "                       .csv(\"imdb_data/\" + ratings_fn)\n",
    "df_ratings = df_ratings.selectExpr(\"tconst\", \n",
    "                                   \"cast(averageRating as float) averageRating\", \n",
    "                                   \"cast(numVotes as int) numVotes\")\n",
    "df_ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writting into a .parquet file : \n",
    "df_ratings.write.mode('overwrite').parquet('parquets/ratings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|              genres|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|tt0000001|    short|          Carmencita|          Carmencita|      0|     1894|     \\N|           1.0|[Documentary, Short]|\n",
      "|tt0000002|    short|Le clown et ses c...|Le clown et ses c...|      0|     1892|     \\N|           5.0|  [Animation, Short]|\n",
      "|tt0000003|    short|      Pauvre Pierrot|      Pauvre Pierrot|      0|     1892|     \\N|           4.0|[Animation, Comed...|\n",
      "|tt0000004|    short|         Un bon bock|         Un bon bock|      0|     1892|     \\N|          null|  [Animation, Short]|\n",
      "|tt0000005|    short|    Blacksmith Scene|    Blacksmith Scene|      0|     1893|     \\N|           1.0|     [Comedy, Short]|\n",
      "|tt0000006|    short|   Chinese Opium Den|   Chinese Opium Den|      0|     1894|     \\N|           1.0|             [Short]|\n",
      "|tt0000007|    short|Corbett and Court...|Corbett and Court...|      0|     1894|     \\N|           1.0|      [Short, Sport]|\n",
      "|tt0000008|    short|Edison Kinetoscop...|Edison Kinetoscop...|      0|     1894|     \\N|           1.0|[Documentary, Short]|\n",
      "|tt0000009|    movie|          Miss Jerry|          Miss Jerry|      0|     1894|     \\N|          45.0|           [Romance]|\n",
      "|tt0000010|    short|Employees Leaving...|La sortie de l'us...|      0|     1895|     \\N|           1.0|[Documentary, Short]|\n",
      "|tt0000011|    short|Akrobatisches Pot...|Akrobatisches Pot...|      0|     1895|     \\N|           1.0|[Documentary, Short]|\n",
      "|tt0000012|    short|The Arrival of a ...|L'arrivée d'un tr...|      0|     1896|     \\N|           1.0|[Documentary, Short]|\n",
      "|tt0000013|    short|The Photographica...|Neuville-sur-Saôn...|      0|     1895|     \\N|           1.0|[Documentary, Short]|\n",
      "|tt0000014|    short|Tables Turned on ...|   L'arroseur arrosé|      0|     1895|     \\N|           1.0|     [Comedy, Short]|\n",
      "|tt0000015|    short| Autour d'une cabine| Autour d'une cabine|      0|     1894|     \\N|           2.0|  [Animation, Short]|\n",
      "|tt0000016|    short|Barque sortant du...|Barque sortant du...|      0|     1895|     \\N|           1.0|[Documentary, Short]|\n",
      "|tt0000017|    short|Italienischer Bau...|Italienischer Bau...|      0|     1895|     \\N|           1.0|[Documentary, Short]|\n",
      "|tt0000018|    short|Das boxende Känguruh|Das boxende Känguruh|      0|     1895|     \\N|           1.0|             [Short]|\n",
      "|tt0000019|    short|    The Clown Barber|    The Clown Barber|      0|     1898|     \\N|          null|     [Comedy, Short]|\n",
      "|tt0000020|    short|      The Derby 1895|      The Derby 1895|      0|     1895|     \\N|           1.0|[Documentary, Sho...|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to split genres\n",
    "udf_split = psf.udf(lambda s: s.split(',') if s is not None else \"\", ArrayType(StringType()))\n",
    "\n",
    "df_basics = spark.read.option(\"header\", \"true\")\\\n",
    "                      .option(\"sep\", \"\\t\")\\\n",
    "                      .csv(\"imdb_data/\" + basics_fn)\n",
    "df_basics = df_basics.withColumn(\"rttmp\", df_basics.runtimeMinutes.cast(DoubleType()))\\\n",
    "                     .drop(\"runtimeMinutes\")\\\n",
    "                     .withColumnRenamed(\"rttmp\", \"runtimeMinutes\")\\\n",
    "                     .withColumn(\"gtmp\", udf_split(\"genres\"))\\\n",
    "                     .drop(\"genres\")\\\n",
    "                     .withColumnRenamed(\"gtmp\", \"genres\")\n",
    "df_basics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# writting into a .parquet file : \n",
    "df_basics.write.mode('overwrite').parquet('parquets/basics.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## OpenSubtitles dataset\n",
    "\n",
    "The dataset consists of 31 GB of XML files distributed in the following file structure: \n",
    "\n",
    "```\n",
    "├── opensubtitle\n",
    "│   ├── OpenSubtitles2018\n",
    "│   │   ├── Year\n",
    "│   │   │   ├── Id\n",
    "│   │   │   │   ├── #######.xml.gz\n",
    "│   │   │   │   ├── #######.xml.gz\n",
    "│   ├── en.tar.gz\n",
    "│   ├── fr.tar.gz\n",
    "│   ├── zh_cn.tar.gz\n",
    "```\n",
    "where\n",
    "- `######` is a 6-digit unique identifier of the file on the OpenSubtitles dataset.\n",
    "- `Year` is the year the movie or episode was made.\n",
    "- `Id` is a 5 to 7 digit identifier (if it's 7-digit it's also an IMDb identifier).\n",
    "\n",
    "The subtitles are provided in different languages. We only analyze the `OpenSubtitles2018` folder and it's the only folder we detail.\n",
    "\n",
    "The decompressed XML files vary in size, ranging from 5KB to 9000KB sized files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML Files\n",
    "\n",
    "Each XML file is split into a `document` and `metadata` section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtitles\n",
    "\n",
    "The `document` section contains all the subtitles and its general structure is the following:\n",
    "\n",
    "```\n",
    "├── s\n",
    "│   ├── time: Integer\n",
    "│   ├── w: String\n",
    "```\n",
    "\n",
    "An example snippet of an XML file:\n",
    "\n",
    "```xml\n",
    "  <s id=\"1\">\n",
    "    <time id=\"T1S\" value=\"00:00:51,819\" />\n",
    "    <w id=\"1.1\">Travis</w>\n",
    "    <w id=\"1.2\">.</w>\n",
    "    <time id=\"T1E\" value=\"00:00:53,352\" />\n",
    "  </s>\n",
    "```\n",
    "\n",
    "The subtitles in each XML file are stored by **blocks** denoted by `s` with a unique `id` attribute (integers in increasing order starting at 1).  \n",
    "\n",
    "Each block (`<s id=\"1\">` for instance) has a:  \n",
    "\n",
    "1. Set of timestamps (denoted by `time`) with\n",
    " - A timestamp `id` attribute that can take two different formats: `T#S` or `T#E`, where _S_ indicates _start_, _E_ indicates _end_ and _#_ is an increasing integer. \n",
    " - A `value` attribute which has the format `HH:mm:ss,fff`.\n",
    "\n",
    "2. Set of words (denoted by `w`) with\n",
    " - an `id` attribute that is simply an increasing number of decimal numbers of the format `X.Y` where X is the string id and Y is the word id within the corresponding string\n",
    " - a non-empty `value` attribute that contains a token: a word or a punctuation character. \n",
    "\n",
    "It sometimes also has an `alternative`, `initial` and `emphasis` attribute.  \n",
    "\n",
    " - The `initial` attribute generally corresponds to slang words or mispronounced words because of an accent such as _lyin'_ instead of _lying_.  \n",
    " - The `alternative` attribute is another way of displaying the subtitle for example _HOW_ instead of _how_.\n",
    " - The `emphasis` attribute is a boolean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "The `metadata` section has the following structure:\n",
    "\n",
    "```\n",
    "├── Conversion\n",
    "│   ├── corrected_words: Integer\n",
    "│   ├── sentences: Integer\n",
    "│   ├── tokens: Integer\n",
    "│   ├── encoding: String (always utf-8)\n",
    "│   ├── unknown_words: Integer\n",
    "│   ├── ignored_blocks: Integer\n",
    "│   ├── truecased_words: Integer\n",
    "├── Subtitle\n",
    "│   ├── language: String\n",
    "│   ├── date: String\n",
    "│   ├── duration: String\n",
    "│   ├── cds: String (presented as #/# where # is an int)\n",
    "│   ├── blocks: Integer\n",
    "│   ├── confidence: Double\n",
    "├── Source\n",
    "│   ├── genre: String[] (up to 3 genres)\n",
    "│   ├── year: Integer\n",
    "│   ├── duration: Integer (in minutes)\n",
    "│   ├── original: String\n",
    "│   ├── country: String\n",
    "```\n",
    "\n",
    "We note that some XML files may not have all the entries. \n",
    "We can use the metadata to obtain additional information about the movie or show's subtitles and compute certain statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "Going through the dataset we notice a few things:\n",
    "\n",
    "1. The dataset has meaningless folders. For example, the folder 1858/ is empty.\n",
    "2. Dataset contains XML files that are not related to movies or TV shows. For example, the folder 666/ contains Justin Bieber song subtitles.  \n",
    "3. Trailer of films can be present in the dataset. For example, the folder 2018/ we found for example Black Panther teaser trailer subtitles.\n",
    "4. Each movie might have more than 1 subtitle file.\n",
    "5. Some subtitle files contain text that is not related to the movie, like credits to the person who made the subtitles.\n",
    "6. The IDMDb folder name is not always a 7-digit number, meaning it is not always a valid IMDb identifer and we can't retrieve the IMDb info.\n",
    "7. Each block may have an arbitrary number (including 0) of timestamps associated to it.\n",
    "\n",
    "To solve points 1 and 2, we ignore all the folders which aren't inside the range of 1920-2018.\n",
    "\n",
    "To solve point 3, we drop trailers by looking at the `duration` field in the metadata section.\n",
    "\n",
    "To solve point 4, we simply take the first one.\n",
    "\n",
    "To solve point 6, we keep movies that have a correct IMDb identifier. Hence, all the files in folders that don't have a 7-digit folder name are dropped.\n",
    "\n",
    "To solve point 7, we decide not to associate a timestamp to each word for the moment.\n",
    " \n",
    "For the moment, we take a sample of the dataset from the cluster (see python script `extract_sample_2.py`) by collecting 1 or 2 movies for each year in the range 1920-2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing an analysis of the files and considering the statistics we want to obtain taking the size of our data into account, we decide to load the metadata and subtitles directly into 1 dataframe where we manipulate it as before. We decide not to extract all tokens at first as it would induce into very heavy computations. We store the text in an array of subtitles where each subtitle is an array of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: long (nullable = true)\n",
      " |-- meta: struct (nullable = true)\n",
      " |    |-- conversion: struct (nullable = true)\n",
      " |    |    |-- corrected_words: long (nullable = true)\n",
      " |    |    |-- encoding: string (nullable = true)\n",
      " |    |    |-- ignored_blocks: long (nullable = true)\n",
      " |    |    |-- sentences: long (nullable = true)\n",
      " |    |    |-- tokens: long (nullable = true)\n",
      " |    |    |-- truecased_words: long (nullable = true)\n",
      " |    |    |-- unknown_words: long (nullable = true)\n",
      " |    |-- source: struct (nullable = true)\n",
      " |    |    |-- duration: long (nullable = true)\n",
      " |    |    |-- genre: string (nullable = true)\n",
      " |    |    |-- year: long (nullable = true)\n",
      " |    |-- subtitle: struct (nullable = true)\n",
      " |    |    |-- blocks: long (nullable = true)\n",
      " |    |    |-- cds: string (nullable = true)\n",
      " |    |    |-- confidence: double (nullable = true)\n",
      " |    |    |-- date: string (nullable = true)\n",
      " |    |    |-- duration: string (nullable = true)\n",
      " |    |    |-- language: string (nullable = true)\n",
      " |-- s: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _emphasis: boolean (nullable = true)\n",
      " |    |    |-- _id: long (nullable = true)\n",
      " |    |    |-- time: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |-- _id: string (nullable = true)\n",
      " |    |    |    |    |-- _value: string (nullable = true)\n",
      " |    |    |-- w: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |    |-- _alternative: string (nullable = true)\n",
      " |    |    |    |    |-- _emphasis: boolean (nullable = true)\n",
      " |    |    |    |    |-- _id: double (nullable = true)\n",
      "\n",
      "+-------+--------------------+--------------------+\n",
      "|    _id|                meta|                   s|\n",
      "+-------+--------------------+--------------------+\n",
      "|6887453|[[0, utf-8, 2, 96...|[[, 1, [[, T1S, 0...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_id = '6464116'\n",
    "df_document_example = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "                                     .options(rowTag='document') \\\n",
    "                                     .load('sample_dataset/2017/6464116/6887453.xml.gz')\n",
    "df_document_example.printSchema()\n",
    "df_document_example.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid confusion, we will set some naming conventions. We will refer to certain attributes as follows:\n",
    "\n",
    "- The `s` array as **blocks**\n",
    "- An element of blocks, as a **block**.\n",
    "- The `w` array as **elements**\n",
    "- An element of elements, as **element**.\n",
    "- `_VALUE` as a **token**\n",
    "- A **subtitle** is a list of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that retrieves the tokens from the elements (`w` array) and returns an array of subtitles, where each subtitle is a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_subtitles_array(sentences):\n",
    "    \"\"\"Function to map the elements (a struct containing tokens)\n",
    "    to a list of list of tokens \"\"\"\n",
    "    s_list = []\n",
    "    if sentences is None:\n",
    "        return s_list\n",
    "    for words in sentences:\n",
    "        w_list = []\n",
    "        if words and \"w\" in words and words[\"w\"]:\n",
    "            for w in words[\"w\"]:\n",
    "                if '_VALUE' in w and w['_VALUE']:\n",
    "                    w_list.append(w['_VALUE'])\n",
    "                \n",
    "            s_list.append(w_list)\n",
    "\n",
    "    return s_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a couple of udf functions we will later use for the manipulation of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to spark function\n",
    "udf_subtitles_array = psf.udf(to_subtitles_array, ArrayType(ArrayType(StringType())))\n",
    "# Convert array of words into a single string\n",
    "udf_sentence = psf.udf(lambda x: ' '.join(x), StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks if correct schema\n",
    "def has_correct_schema(df):\n",
    "    arguments = [\n",
    "                 \"meta.conversion.sentences\",\n",
    "                 \"meta.source.year\", \n",
    "                 \"meta.subtitle.blocks\",\n",
    "                 \"meta.subtitle.duration\",\n",
    "                 \"meta.subtitle.language\",\n",
    "                 \"s\"]\n",
    "    for col in arguments:\n",
    "        try:\n",
    "            df[col]\n",
    "        except AnalysisException:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "schema_films = StructType([StructField('tconst', StringType(), False),\n",
    "                               StructField('num_sentences', LongType(), True),\n",
    "                               StructField('year', LongType(), True),\n",
    "                               StructField('blocks', LongType(), True),\n",
    "                               StructField('subtitle_mins', DoubleType(), True),\n",
    "                               StructField('subtitles', ArrayType(ArrayType(StringType())), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below structures our data to the format we want to then process all the queries we need: We link the movie with the proper imdbID, we get all the subtitles, change the subtitle duration to be in seconds (We assume for this that they all have the same format and after exploring the dataset we know the vast majority does)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_correct_schema(df_document_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df_document, imdb_id):\n",
    "    \"\"\"Restructures and selects the columns of a dataframe of an XML\n",
    "    file with its corresponding IMDB Id\"\"\"\n",
    "    # Create IMDb ID and subtitles column\n",
    "    df_film_sentences = df_document.withColumn(\"tconst\", psf.lit(\"tt\" + imdb_id))\\\n",
    "                                   .withColumn(\"subtitles\", udf_subtitles_array(\"s\"))\n",
    "    \n",
    "    # Select metadata and previously created columns\n",
    "    df_result = df_film_sentences.selectExpr(\"tconst\",\n",
    "                                             \"meta.conversion.sentences as num_sentences\",\n",
    "                                             \"meta.source.year\", \n",
    "                                             \"meta.subtitle.blocks\",\n",
    "                                             \"meta.subtitle.duration as subtitle_duration\",\n",
    "                                             \"meta.subtitle.language\",\n",
    "                                             \"subtitles\")\n",
    "    # Split genre column and convert subtitle duration to seconds\n",
    "    df_result = df_result.withColumn(\"subtitle_mins\", \n",
    "                                     psf.unix_timestamp(df_result.subtitle_duration, \"HH:mm:ss,SSS\") / 60)\n",
    "    # Discard redundant columns\n",
    "    return df_result.select(\"tconst\", \"num_sentences\", \"year\", \"blocks\", \"subtitle_mins\", \"subtitles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying it on the example XML file, we see how our corresponding schema is much more simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = false)\n",
      " |-- num_sentences: long (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- blocks: long (nullable = true)\n",
      " |-- subtitle_mins: double (nullable = true)\n",
      " |-- subtitles: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n",
      "+---------+-------------+----+------+-------------+--------------------+\n",
      "|   tconst|num_sentences|year|blocks|subtitle_mins|           subtitles|\n",
      "+---------+-------------+----+------+-------------+--------------------+\n",
      "|tt6464116|          967|2017|   888|         40.7|[[[, shots, firin...|\n",
      "+---------+-------------+----+------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_document_example = clean_df(df_document_example, imdb_id)\n",
    "df_document_example.printSchema()\n",
    "df_document_example.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generalize what we have done so far. We would like to create a dataframe for several XML files, so we define a function that does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(path):\n",
    "    \"\"\"Load an XML subtitles file into a dataframe\"\"\"\n",
    "    df_film = sqlContext.read.format('com.databricks.spark.xml')\\\n",
    "                             .options(rowTag='document')\\\n",
    "                             .load(path)\n",
    "    return df_film"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a dataframe for all the XML files in the sample dataset. We will later expand it to cover a bigger quantity of films. We call it `df_films` because it contains all the information for each film/show in our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#path = \"sample_dataset/\"\n",
    "# Create empty dataframe with same schema\n",
    "\n",
    "\n",
    "#df_films = spark.createDataFrame([], schema_films)\n",
    "#film_list = []\n",
    "#for year in os.listdir(path):\n",
    " #   if not year.startswith('.'):\n",
    "  #      for imdb_id in os.listdir(path + year):\n",
    "   #         if not imdb_id.startswith('.'):\n",
    "    #            current_path = path + year + \"/\" + imdb_id\n",
    "     #           for idx, file in enumerate(os.listdir(current_path)):\n",
    "      #                  # Create a dataframe for each file\n",
    "       #                 df_document = load_df(current_path + '/' + file)\n",
    "        #                if has_correct_schema(df_document):\n",
    "                            # Restructure dataframe and add it to df_films\n",
    "         #                   film_list.append(clean_df(df_document, imdb_id))\n",
    "                        \n",
    "\n",
    "#             df_m.show()\n",
    "#             print(current_path + \"/\" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(*dfs):\n",
    "    first, rest = dfs[0], dfs[1:]  # Python 3.x, for 2.x you'll have to unpack manually\n",
    "    return first.sql_ctx.createDataFrame(\n",
    "        first.sql_ctx._sc.union([df.rdd for df in dfs]),\n",
    "        first.schema\n",
    "    )\n",
    "#df_films = unionAll(*film_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same structure to create dataframes which contain the entries we want to focus on, more specifically movies with more than 5000 reviews. The script can be found under the name `cluster/parquet2.py`. To filter the appropiate files we used the imdb datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_films = spark.read.parquet(\"parquets/films.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_list = [spark.read.parquet(\"parquets/\" + path) for path in os.listdir(\"parquets\") if path != \".DS_Store\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total films we will be focusing on then is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4286"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_films = unionAll(*df_list)\n",
    "df_films.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_films.write.parquet(\"parquets/films.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+----+------+------------------+--------------------+\n",
      "|   tconst|num_subtitles|year|blocks|     subtitle_mins|           subtitles|\n",
      "+---------+-------------+----+------+------------------+--------------------+\n",
      "|tt5275892|         6236|2016|  1829|            406.05|[[As, a, kid, gro...|\n",
      "|tt2234155|         3562|2013|  3348|63.166666666666664|[[An, old, man, t...|\n",
      "|tt2318527|         7083|2013|  2793| 32.46666666666667|[[BELL, TOLLING],...|\n",
      "|tt2404463|         4078|2013|  3356| 60.11666666666667|[[Cleaned, correc...|\n",
      "|tt3311384|         1993|2013|  2116|59.016666666666666|[[CROWD, CHEERING...|\n",
      "|tt1398426|         3729|2015|  2768| 86.06666666666666|[[Police, Radio, ...|\n",
      "|tt4540710|         2018|2016|  1668| 67.23333333333333|[[Subtitle, made,...|\n",
      "|tt4257858|         1553|2015|  2415|              58.8|[[We, will, begin...|\n",
      "|tt2080374|         2889|2015|  2309|             62.15|[[MAN], [And, wit...|\n",
      "|tt4425064|         1807|2015|  1951| 68.61666666666666|[[After, hours, o...|\n",
      "|tt4987556|         2916|2015|  2560| 97.51666666666667|[[Define, your, e...|\n",
      "|tt1638364|         1864|2013|  2272|             61.15|[[SUBTITLES, BY],...|\n",
      "|tt1824254|         1364|2013|  2004|             67.05|[[groups, go, to,...|\n",
      "|tt3152624|         3926|2015|  2508| 62.78333333333333|[[Girls, your, mo...|\n",
      "|tt1229340|         3055|2013|  2622|              62.7|[[NARRATOR], [The...|\n",
      "|tt1980209|         2866|2013|  2517| 61.78333333333333|[[I, strong], [I,...|\n",
      "|tt5213534|         2090|2016|  2112|55.733333333333334|[[INTERVIEWER], [...|\n",
      "|tt3628584|         3018|2016|  2784|             49.15|[[Yes, sir, it, C...|\n",
      "|tt0884732|         3095|2015|  2560|             41.35|[[DIALING, PHONE]...|\n",
      "|tt5895028|         1215|2016|  1730| 39.68333333333333|[[Barack, Obama, ...|\n",
      "+---------+-------------+----+------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- num_subtitles: long (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- blocks: long (nullable = true)\n",
      " |-- subtitle_mins: double (nullable = true)\n",
      " |-- subtitles: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_films.show()\n",
    "df_films.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "Here we discuss how we want to proceed with our data.\n",
    "\n",
    "We consider only films with more than 5,000 reviews as IMDb considers this a good metric to estimate the public actual approval of a film.\n",
    "\n",
    "We first start by analyzing the 1000 best films according to IMDB with more that 20000 reviews. We look into the resulting dataframe columns and  try to look for some kind of pattern. We look at: \n",
    "- the ratio of subtitle time per runtime, \n",
    "- the most used words in the films, \n",
    "- the ratio of distinct words per total number of words, \n",
    "- average sentence length, \n",
    "- number of sentences. \n",
    "\n",
    "We draw conclusions if we see any apparent relation. Afterwards if we can find a relation between rating and one of the resulting values, we do the same tests on the worst imdb films with more than 20000 reviews.. If we find that a relation holds for both groups we make a statement assuming there exists a relationship\n",
    "\n",
    "We know this is very vague and we cannot make a strong statement based on what we find, if we find anything because we don't take into consideration many variables such as the movie genre, the year of release, if it is an adult film or not. Also the country of origin is a variable to take into account but we don't have it yet. We can also check the box office for the best films.\n",
    "\n",
    "To proceed then, we ask what are the most popular genres and we look into how the statistics for the 10 most popular ones behave. Can we find any relationship between a genre and the statistics. We look aswell for relationships between genre and rating. Is the genre of a movie a variable which influences average rating. \n",
    "\n",
    "It is all interconnected, we ask ourselves for example, given an action movie, are the value of the statistics influential to its average rating. \n",
    "\n",
    "We havent taken TIME into account either! We look into different time periods, particular years where something occured (WW2 end or smth like that) and we try using the same statistics, to find relationships. We need to choose how to address the other variables such as genre and if it is an adult film or not.\n",
    "\n",
    "After analyzing this data if we find that there are actually dependencies between the values, we decide to create a small learning algorithm to predict the rating of a film given its script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_statistics(df_films):\n",
    "    df_words = df_films.select('*', psf.explode('subtitles').alias('words'))\n",
    "    df_words = df_words.select('*', psf.explode('words').alias('word'))\n",
    "    df_basics \n",
    "    \n",
    "    df_word_counts = df_words.groupby(\"tconst\", \"num_subtitles\", \"blocks\", \"subtitle_mins\")\\\n",
    "                             .agg(psf.count(\"word\").alias(\"Number of words\"), \n",
    "                                  psf.countDistinct(\"word\").alias(\"Number of distinct words\"))\\\n",
    "                             .withColumn(\"Mean length of sentences\",\n",
    "                                         psf.col(\"Number of words\") / psf.col(\"num_subtitles\"))\\\n",
    "                             .withColumn(\"Spread of sentences\",\n",
    "                                         psf.col(\"num_subtitles\") / psf.col(\"blocks\"))\\\n",
    "                             .withColumn(\"Spread of sentences\",\n",
    "                                         psf.col(\"num_subtitles\") / psf.col(\"blocks\"))\\\n",
    "                             .join(df_basics, ['tconst'], 'left_outer')\\\n",
    "                             .withColumn(\"Runtime to Subtitles minutes\",\n",
    "                                         psf.col(\"subtitle_mins\")/ psf.col(\"runtimeMinutes\"))\n",
    "#                             .select(df_word_counts['*'], \n",
    "#                                      (df_basics['runtimeMinutes'] / df_word_counts['subtitle_mins'])\\\n",
    "#                                      .alias(\"Runtime to Subtitles minutes\"))\n",
    "    \n",
    "    return df_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+------+------------------+---------------+------------------------+------------------------+-------------------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+----------------------------+\n",
      "|   tconst|num_subtitles|blocks|     subtitle_mins|Number of words|Number of distinct words|Mean length of sentences|Spread of sentences|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|              genres|Runtime to Subtitles minutes|\n",
      "+---------+-------------+------+------------------+---------------+------------------------+------------------------+-------------------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+----------------------------+\n",
      "|tt0032553|         1482|  1268| 59.71666666666667|           7294|                    1753|       4.921727395411606| 1.1687697160883281|    movie|  The Great Dictator|  The Great Dictator|      0|     1940|     \\N|         125.0|[Comedy, Drama, War]|         0.47773333333333334|\n",
      "|tt0032599|         2883|  1973|             31.35|          14828|                    2187|       5.143253555324315|  1.461226558540294|    movie|     His Girl Friday|     His Girl Friday|      0|     1940|     \\N|          92.0|[Comedy, Drama, R...|          0.3407608695652174|\n",
      "|tt0036323|         1272|   499|             33.85|           7253|                    1494|       5.702044025157233| 2.5490981963927855|    movie|              Sahara|              Sahara|      0|     1943|     \\N|          97.0|[Action, Drama, War]|          0.3489690721649485|\n",
      "|tt0039305|         1597|  1148|              36.1|           9832|                    1876|       6.156543519098309| 1.3911149825783973|    movie|      Dead Reckoning|      Dead Reckoning|      0|     1947|     \\N|         100.0|[Crime, Drama, Fi...|                       0.361|\n",
      "|tt0063688|         1033|   768| 40.68333333333333|           4469|                    1173|      4.3262342691190705| 1.3450520833333333|    movie|The Thomas Crown ...|The Thomas Crown ...|      0|     1968|     \\N|         102.0|[Crime, Drama, Ro...|         0.39885620915032677|\n",
      "|tt0066193|         1729|  1023| 32.93333333333333|           9731|                    1615|       5.628108733371891| 1.6901270772238515|    movie|  The Out of Towners|  The Out of Towners|      0|     1970|     \\N|         101.0|            [Comedy]|         0.32607260726072607|\n",
      "|tt0100142|         1940|  1517| 36.21666666666667|          12914|                    2178|       6.656701030927835| 1.2788398154251812|    movie|        Metropolitan|        Metropolitan|      0|     1990|     \\N|          98.0|[Comedy, Drama, R...|         0.36955782312925173|\n",
      "|tt0102216|         1561|  1446| 33.38333333333333|           8971|                    2034|       5.746957078795644| 1.0795297372060857|    movie|          King Ralph|          King Ralph|      0|     1991|     \\N|          97.0|   [Comedy, Romance]|         0.34415807560137457|\n",
      "|tt0107840|         1737|  1085| 43.88333333333333|           8618|                    1456|       4.961427748992516| 1.6009216589861752|    movie|      Poetic Justice|      Poetic Justice|      0|     1993|     \\N|         109.0|    [Drama, Romance]|          0.4025993883792049|\n",
      "|tt0117718|         1114|  1183|52.333333333333336|           7483|                    1369|       6.717235188509874| 0.9416737109044802|    movie|  The Spitfire Grill|  The Spitfire Grill|      0|     1996|     \\N|         117.0|             [Drama]|         0.44729344729344733|\n",
      "|tt0204946|         1823|  1308|26.733333333333334|           8789|                    1938|       4.821173889193637|  1.393730886850153|    movie|         Bring It On|         Bring It On|      0|     2000|     \\N|          98.0|[Comedy, Romance,...|          0.2727891156462585|\n",
      "|tt0383028|         2065|  1484|              60.3|          10193|                    1825|       4.936077481840194| 1.3915094339622642|    movie|Synecdoche, New York|Synecdoche, New York|      0|     2008|     \\N|         124.0|     [Comedy, Drama]|         0.48629032258064514|\n",
      "|tt0449010|          916|   879|             32.95|           4608|                    1106|        5.03056768558952| 1.0420932878270763|    movie|              Eragon|              Eragon|      0|     2006|     \\N|         104.0|[Action, Adventur...|          0.3168269230769231|\n",
      "|tt0456165|         2886|  1173| 87.03333333333333|          14974|                    2116|       5.188496188496188| 2.4603580562659846|    movie|      Salaam Namaste|      Salaam Namaste|      0|     2005|     \\N|         158.0|[Comedy, Drama, R...|          0.5508438818565401|\n",
      "|tt0463854|         1033|   734|29.916666666666668|           3791|                    1023|       3.669893514036786| 1.4073569482288828|    movie|      28 Weeks Later|      28 Weeks Later|      0|     2007|     \\N|         100.0|[Drama, Horror, S...|          0.2991666666666667|\n",
      "|tt0464049|         1989|   793|             47.65|          11361|                    2555|       5.711915535444947| 2.5081967213114753|    movie|    The History Boys|    The History Boys|      0|     2006|     \\N|         109.0|[Comedy, Drama, R...|          0.4371559633027523|\n",
      "|tt1031969|         2242|   651| 37.88333333333333|           9699|                    1934|       4.326048171275647| 3.4439324116743473|    movie|          The Rocker|          The Rocker|      0|     2008|     \\N|         102.0|     [Comedy, Music]|         0.37140522875816995|\n",
      "|tt1188990|          486|   352|22.066666666666666|           2122|                     698|       4.366255144032922| 1.3806818181818181|    movie|         Cold Prey 2|       Fritt vilt II|      0|     2008|     \\N|          86.0|  [Horror, Thriller]|          0.2565891472868217|\n",
      "|tt1280558|         1552|   564|            123.85|           8903|                    1595|       5.736469072164948|   2.75177304964539|    movie|         A Wednesday|         A Wednesday|      0|     2008|     \\N|         104.0|[Crime, Drama, My...|          1.1908653846153845|\n",
      "|tt1529572|         1833|  1606| 39.06666666666667|           9051|                    1560|       4.937806873977086| 1.1413449564134495|    movie|               Trust|               Trust|      0|     2010|     \\N|         106.0|[Crime, Drama, Th...|          0.3685534591194969|\n",
      "+---------+-------------+------+------------------+---------------+------------------------+------------------------+-------------------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_word_stats = word_statistics(df_films)\n",
    "df_word_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o442.showString.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \r\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:136)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:367)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:135)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:232)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\r\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:35)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:65)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\r\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:354)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:383)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\r\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:354)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\r\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\r\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:524)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:576)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:337)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 63.0 failed 1 times, most recent failure: Lost task 0.0 in stage 63.0 (TID 2787, localhost, executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:304)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:76)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:73)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:97)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\r\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\r\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-78cb1c4598dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf_words_ratings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_word_stats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_ratings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tconst\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_words_ratings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    348\u001b[0m         \"\"\"\n\u001b[0;32m    349\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o442.showString.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \r\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:136)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:367)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:135)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:232)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\r\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:35)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:65)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\r\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:354)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:383)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\r\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:354)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\r\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\r\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:524)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:576)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:337)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 63.0 failed 1 times, most recent failure: Lost task 0.0 in stage 63.0 (TID 2787, localhost, executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:304)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:76)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:73)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:97)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\r\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\r\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "df_words_ratings = df_word_stats.join(df_ratings, \"tconst\")\n",
    "df_words_ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words_ratings_pd = df_words_ratings.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_words_ratings_pd.plot.scatter(\"num_subtitles\", \"averageRating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_words_ratings_pd.plot.scatter(\"blocks\", \"averageRating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_words_ratings_pd.plot.scatter(\"subtitle_mins\", \"averageRating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_words_ratings_pd.plot.scatter(\"Number of words\", \"averageRating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_words_ratings_pd.plot.scatter(\"Number of distinct words\", \"averageRating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_words_ratings_pd.plot.scatter(\"Mean length of sentences\", \"averageRating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_words_ratings_pd.plot.scatter(\"Spread of sentences\", \"averageRating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_words_ratings_pd.plot.scatter(\"Runtime to Subtitles minutes\", \"averageRating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_ratings.filter(df_ratings.numVotes > 20000)\n",
    "df_50_best = df_films.join(df_filtered, [\"tconst\"])\\\n",
    "                     .orderBy(df_ratings.averageRating.desc())\\\n",
    "                     .select(\"num_sentences\", \n",
    "                             \"averageRating\", \n",
    "                             \"numVotes\", \"year\", \"tconst\")\\\n",
    "                     .take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cest de la merde pour linstant vue la taille du dataset\n",
    "df_pd_example = pd.DataFrame(df_50_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
